{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM1: Logistic Regression.\n",
    "\n",
    "### Name: [Ayomide Akinsanya]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also you should plot their objective values versus epochs and compare their training and testing accuracies. You will need to tune the parameters a little bit to obtain reasonable results.\n",
    "\n",
    "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy \n",
    "import math\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "- Load the data.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAST_CANCER_PATH = os.path.join(\"CS583 Deep Learning\",\"Datasets\",\"Breats Cancer\")\n",
    "\n",
    "def load_breast_cancer_data(breast_cancer_path=BREAST_CANCER_PATH):\n",
    "    file_path = os.path.join(breast_cancer_path,\"data.csv\")\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = load_breast_cancer_data()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Examine and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
    "# You need to get rid of the ID number feature.\n",
    "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "breast_cancer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
       "count     569.000000  ...     569.000000       569.000000   569.000000   \n",
       "mean        0.181162  ...      25.677223       107.261213   880.583128   \n",
       "std         0.027414  ...       6.146258        33.602542   569.356993   \n",
       "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
       "25%         0.161900  ...      21.080000        84.110000   515.300000   \n",
       "50%         0.179200  ...      25.410000        97.660000   686.500000   \n",
       "75%         0.195700  ...      29.720000       125.400000  1084.000000   \n",
       "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
       "\n",
       "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "count        569.000000         569.000000       569.000000   \n",
       "mean           0.132369           0.254265         0.272188   \n",
       "std            0.022832           0.157336         0.208624   \n",
       "min            0.071170           0.027290         0.000000   \n",
       "25%            0.116600           0.147200         0.114500   \n",
       "50%            0.131300           0.211900         0.226700   \n",
       "75%            0.146000           0.339100         0.382900   \n",
       "max            0.222600           1.058000         1.252000   \n",
       "\n",
       "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "count            569.000000      569.000000               569.000000   \n",
       "mean               0.114606        0.290076                 0.083946   \n",
       "std                0.065732        0.061867                 0.018061   \n",
       "min                0.000000        0.156500                 0.055040   \n",
       "25%                0.064930        0.250400                 0.071460   \n",
       "50%                0.099930        0.282200                 0.080040   \n",
       "75%                0.161400        0.317900                 0.092080   \n",
       "max                0.291000        0.663800                 0.207500   \n",
       "\n",
       "       Unnamed: 32  \n",
       "count          0.0  \n",
       "mean           NaN  \n",
       "std            NaN  \n",
       "min            NaN  \n",
       "25%            NaN  \n",
       "50%            NaN  \n",
       "75%            NaN  \n",
       "max            NaN  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-1</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0            1        17.99         10.38          122.80     1001.0   \n",
       "1            1        20.57         17.77          132.90     1326.0   \n",
       "2            1        19.69         21.25          130.00     1203.0   \n",
       "3            1        11.42         20.38           77.58      386.1   \n",
       "4            1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564          1        21.56         22.39          142.00     1479.0   \n",
       "565          1        20.13         28.25          131.20     1261.0   \n",
       "566          1        16.60         28.08          108.30      858.1   \n",
       "567          1        20.60         29.33          140.10     1265.0   \n",
       "568         -1         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer = breast_cancer.drop([\"id\",\"Unnamed: 32\"],axis=1)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder= OrdinalEncoder()\n",
    "breast_cancer_encoded = ordinal_encoder.fit_transform(breast_cancer[[\"diagnosis\"]])\n",
    "breast_cancer[\"diagnosis\"] = breast_cancer_encoded\n",
    "breast_cancer[\"diagnosis\"].replace({0.0:-1,1.0:1},inplace=True)\n",
    "breast_cancer[\"diagnosis\"] = breast_cancer[\"diagnosis\"].astype(int)\n",
    "\n",
    "breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Partition to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    numpy.random.seed(42)\n",
    "    random_indices = numpy.random.permutation(len(data))\n",
    "    test_set_size = int(len(data)*test_ratio)\n",
    "    test_indices = random_indices[:test_set_size]\n",
    "    train_indices = random_indices[test_set_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machinel learning.\n",
    "train_set, test_set = split_train_test(breast_cancer,0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1</td>\n",
       "      <td>20.730</td>\n",
       "      <td>31.12</td>\n",
       "      <td>135.70</td>\n",
       "      <td>1419.0</td>\n",
       "      <td>0.09469</td>\n",
       "      <td>0.11430</td>\n",
       "      <td>0.13670</td>\n",
       "      <td>0.08646</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>...</td>\n",
       "      <td>32.490</td>\n",
       "      <td>47.16</td>\n",
       "      <td>214.00</td>\n",
       "      <td>3432.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26440</td>\n",
       "      <td>0.34420</td>\n",
       "      <td>0.16590</td>\n",
       "      <td>0.2868</td>\n",
       "      <td>0.08218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-1</td>\n",
       "      <td>9.029</td>\n",
       "      <td>17.33</td>\n",
       "      <td>58.79</td>\n",
       "      <td>250.5</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.31300</td>\n",
       "      <td>0.04375</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>10.310</td>\n",
       "      <td>22.65</td>\n",
       "      <td>65.50</td>\n",
       "      <td>324.7</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.43650</td>\n",
       "      <td>1.25200</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.11750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1</td>\n",
       "      <td>21.090</td>\n",
       "      <td>26.57</td>\n",
       "      <td>142.70</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.24870</td>\n",
       "      <td>0.14960</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>...</td>\n",
       "      <td>26.680</td>\n",
       "      <td>33.48</td>\n",
       "      <td>176.50</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.75840</td>\n",
       "      <td>0.67800</td>\n",
       "      <td>0.29030</td>\n",
       "      <td>0.4098</td>\n",
       "      <td>0.12840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-1</td>\n",
       "      <td>9.173</td>\n",
       "      <td>13.86</td>\n",
       "      <td>59.20</td>\n",
       "      <td>260.9</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.05988</td>\n",
       "      <td>0.02180</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>10.010</td>\n",
       "      <td>19.23</td>\n",
       "      <td>65.59</td>\n",
       "      <td>310.1</td>\n",
       "      <td>0.09836</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.13970</td>\n",
       "      <td>0.05087</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.08490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-1</td>\n",
       "      <td>10.650</td>\n",
       "      <td>25.22</td>\n",
       "      <td>68.01</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.07234</td>\n",
       "      <td>0.02379</td>\n",
       "      <td>0.01615</td>\n",
       "      <td>0.1897</td>\n",
       "      <td>...</td>\n",
       "      <td>12.250</td>\n",
       "      <td>35.19</td>\n",
       "      <td>77.98</td>\n",
       "      <td>455.7</td>\n",
       "      <td>0.14990</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.06136</td>\n",
       "      <td>0.3409</td>\n",
       "      <td>0.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-1</td>\n",
       "      <td>8.888</td>\n",
       "      <td>14.64</td>\n",
       "      <td>58.79</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.15310</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.02872</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>...</td>\n",
       "      <td>9.733</td>\n",
       "      <td>15.67</td>\n",
       "      <td>62.56</td>\n",
       "      <td>284.4</td>\n",
       "      <td>0.12070</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.14340</td>\n",
       "      <td>0.04786</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-1</td>\n",
       "      <td>11.640</td>\n",
       "      <td>18.33</td>\n",
       "      <td>75.17</td>\n",
       "      <td>412.5</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.07070</td>\n",
       "      <td>0.03485</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>...</td>\n",
       "      <td>13.140</td>\n",
       "      <td>29.26</td>\n",
       "      <td>85.51</td>\n",
       "      <td>521.7</td>\n",
       "      <td>0.16880</td>\n",
       "      <td>0.26600</td>\n",
       "      <td>0.28730</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2806</td>\n",
       "      <td>0.09097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-1</td>\n",
       "      <td>14.290</td>\n",
       "      <td>16.82</td>\n",
       "      <td>90.30</td>\n",
       "      <td>632.6</td>\n",
       "      <td>0.06429</td>\n",
       "      <td>0.02675</td>\n",
       "      <td>0.00725</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>20.65</td>\n",
       "      <td>94.44</td>\n",
       "      <td>684.6</td>\n",
       "      <td>0.08567</td>\n",
       "      <td>0.05036</td>\n",
       "      <td>0.03866</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1</td>\n",
       "      <td>13.980</td>\n",
       "      <td>19.62</td>\n",
       "      <td>91.12</td>\n",
       "      <td>599.5</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.06463</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>...</td>\n",
       "      <td>17.040</td>\n",
       "      <td>30.80</td>\n",
       "      <td>113.90</td>\n",
       "      <td>869.3</td>\n",
       "      <td>0.16130</td>\n",
       "      <td>0.35680</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-1</td>\n",
       "      <td>12.180</td>\n",
       "      <td>20.52</td>\n",
       "      <td>77.22</td>\n",
       "      <td>458.7</td>\n",
       "      <td>0.08013</td>\n",
       "      <td>0.04038</td>\n",
       "      <td>0.02383</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340</td>\n",
       "      <td>32.84</td>\n",
       "      <td>84.58</td>\n",
       "      <td>547.8</td>\n",
       "      <td>0.11230</td>\n",
       "      <td>0.08862</td>\n",
       "      <td>0.11450</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.06878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "265          1       20.730         31.12          135.70     1419.0   \n",
       "68          -1        9.029         17.33           58.79      250.5   \n",
       "181          1       21.090         26.57          142.70     1311.0   \n",
       "63          -1        9.173         13.86           59.20      260.9   \n",
       "248         -1       10.650         25.22           68.01      347.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "71          -1        8.888         14.64           58.79      244.0   \n",
       "106         -1       11.640         18.33           75.17      412.5   \n",
       "270         -1       14.290         16.82           90.30      632.6   \n",
       "435          1       13.980         19.62           91.12      599.5   \n",
       "102         -1       12.180         20.52           77.22      458.7   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "265          0.09469           0.11430         0.13670              0.08646   \n",
       "68           0.10660           0.14130         0.31300              0.04375   \n",
       "181          0.11410           0.28320         0.24870              0.14960   \n",
       "63           0.07721           0.08751         0.05988              0.02180   \n",
       "248          0.09657           0.07234         0.02379              0.01615   \n",
       "..               ...               ...             ...                  ...   \n",
       "71           0.09783           0.15310         0.08606              0.02872   \n",
       "106          0.11420           0.10170         0.07070              0.03485   \n",
       "270          0.06429           0.02675         0.00725              0.00625   \n",
       "435          0.10600           0.11330         0.11260              0.06463   \n",
       "102          0.08013           0.04038         0.02383              0.01770   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "265         0.1769  ...        32.490          47.16           214.00   \n",
       "68          0.2111  ...        10.310          22.65            65.50   \n",
       "181         0.2395  ...        26.680          33.48           176.50   \n",
       "63          0.2341  ...        10.010          19.23            65.59   \n",
       "248         0.1897  ...        12.250          35.19            77.98   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "71          0.1902  ...         9.733          15.67            62.56   \n",
       "106         0.1801  ...        13.140          29.26            85.51   \n",
       "270         0.1508  ...        14.910          20.65            94.44   \n",
       "435         0.1669  ...        17.040          30.80           113.90   \n",
       "102         0.1739  ...        13.340          32.84            84.58   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "265      3432.0           0.14010            0.26440          0.34420   \n",
       "68        324.7           0.14820            0.43650          1.25200   \n",
       "181      2089.0           0.14910            0.75840          0.67800   \n",
       "63        310.1           0.09836            0.16780          0.13970   \n",
       "248       455.7           0.14990            0.13980          0.11250   \n",
       "..          ...               ...                ...              ...   \n",
       "71        284.4           0.12070            0.24360          0.14340   \n",
       "106       521.7           0.16880            0.26600          0.28730   \n",
       "270       684.6           0.08567            0.05036          0.03866   \n",
       "435       869.3           0.16130            0.35680          0.40690   \n",
       "102       547.8           0.11230            0.08862          0.11450   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "265               0.16590          0.2868                  0.08218  \n",
       "68                0.17500          0.4228                  0.11750  \n",
       "181               0.29030          0.4098                  0.12840  \n",
       "63                0.05087          0.3282                  0.08490  \n",
       "248               0.06136          0.3409                  0.08147  \n",
       "..                    ...             ...                      ...  \n",
       "71                0.04786          0.2254                  0.10840  \n",
       "106               0.12180          0.2806                  0.09097  \n",
       "270               0.03333          0.2458                  0.06120  \n",
       "435               0.18270          0.3179                  0.10550  \n",
       "102               0.07431          0.2694                  0.06878  \n",
       "\n",
       "[456 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265    1\n",
      "68    -1\n",
      "181    1\n",
      "63    -1\n",
      "248   -1\n",
      "      ..\n",
      "71    -1\n",
      "106   -1\n",
      "270   -1\n",
      "435    1\n",
      "102   -1\n",
      "Name: diagnosis, Length: 456, dtype: int64\n",
      "204   -1\n",
      "70     1\n",
      "131    1\n",
      "431   -1\n",
      "540   -1\n",
      "      ..\n",
      "148   -1\n",
      "486   -1\n",
      "75     1\n",
      "249   -1\n",
      "238   -1\n",
      "Name: diagnosis, Length: 113, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_train = train_set[\"diagnosis\"]\n",
    "print(y_train)\n",
    "y_test = test_set[\"diagnosis\"]\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "265       20.730         31.12          135.70     1419.0          0.09469   \n",
      "68         9.029         17.33           58.79      250.5          0.10660   \n",
      "181       21.090         26.57          142.70     1311.0          0.11410   \n",
      "63         9.173         13.86           59.20      260.9          0.07721   \n",
      "248       10.650         25.22           68.01      347.0          0.09657   \n",
      "..           ...           ...             ...        ...              ...   \n",
      "71         8.888         14.64           58.79      244.0          0.09783   \n",
      "106       11.640         18.33           75.17      412.5          0.11420   \n",
      "270       14.290         16.82           90.30      632.6          0.06429   \n",
      "435       13.980         19.62           91.12      599.5          0.10600   \n",
      "102       12.180         20.52           77.22      458.7          0.08013   \n",
      "\n",
      "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
      "265           0.11430         0.13670              0.08646         0.1769   \n",
      "68            0.14130         0.31300              0.04375         0.2111   \n",
      "181           0.28320         0.24870              0.14960         0.2395   \n",
      "63            0.08751         0.05988              0.02180         0.2341   \n",
      "248           0.07234         0.02379              0.01615         0.1897   \n",
      "..                ...             ...                  ...            ...   \n",
      "71            0.15310         0.08606              0.02872         0.1902   \n",
      "106           0.10170         0.07070              0.03485         0.1801   \n",
      "270           0.02675         0.00725              0.00625         0.1508   \n",
      "435           0.11330         0.11260              0.06463         0.1669   \n",
      "102           0.04038         0.02383              0.01770         0.1739   \n",
      "\n",
      "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
      "265                 0.05674  ...        32.490          47.16   \n",
      "68                  0.08046  ...        10.310          22.65   \n",
      "181                 0.07398  ...        26.680          33.48   \n",
      "63                  0.06963  ...        10.010          19.23   \n",
      "248                 0.06329  ...        12.250          35.19   \n",
      "..                      ...  ...           ...            ...   \n",
      "71                  0.08980  ...         9.733          15.67   \n",
      "106                 0.06520  ...        13.140          29.26   \n",
      "270                 0.05376  ...        14.910          20.65   \n",
      "435                 0.06544  ...        17.040          30.80   \n",
      "102                 0.05677  ...        13.340          32.84   \n",
      "\n",
      "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
      "265           214.00      3432.0           0.14010            0.26440   \n",
      "68             65.50       324.7           0.14820            0.43650   \n",
      "181           176.50      2089.0           0.14910            0.75840   \n",
      "63             65.59       310.1           0.09836            0.16780   \n",
      "248            77.98       455.7           0.14990            0.13980   \n",
      "..               ...         ...               ...                ...   \n",
      "71             62.56       284.4           0.12070            0.24360   \n",
      "106            85.51       521.7           0.16880            0.26600   \n",
      "270            94.44       684.6           0.08567            0.05036   \n",
      "435           113.90       869.3           0.16130            0.35680   \n",
      "102            84.58       547.8           0.11230            0.08862   \n",
      "\n",
      "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "265          0.34420               0.16590          0.2868   \n",
      "68           1.25200               0.17500          0.4228   \n",
      "181          0.67800               0.29030          0.4098   \n",
      "63           0.13970               0.05087          0.3282   \n",
      "248          0.11250               0.06136          0.3409   \n",
      "..               ...                   ...             ...   \n",
      "71           0.14340               0.04786          0.2254   \n",
      "106          0.28730               0.12180          0.2806   \n",
      "270          0.03866               0.03333          0.2458   \n",
      "435          0.40690               0.18270          0.3179   \n",
      "102          0.11450               0.07431          0.2694   \n",
      "\n",
      "     fractal_dimension_worst  \n",
      "265                  0.08218  \n",
      "68                   0.11750  \n",
      "181                  0.12840  \n",
      "63                   0.08490  \n",
      "248                  0.08147  \n",
      "..                       ...  \n",
      "71                   0.10840  \n",
      "106                  0.09097  \n",
      "270                  0.06120  \n",
      "435                  0.10550  \n",
      "102                  0.06878  \n",
      "\n",
      "[456 rows x 30 columns]\n",
      "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "204        12.47         18.60           81.09      481.9          0.09965   \n",
      "70         18.94         21.31          123.60     1130.0          0.09009   \n",
      "131        15.46         19.48          101.70      748.9          0.10920   \n",
      "431        12.40         17.68           81.47      467.8          0.10540   \n",
      "540        11.54         14.44           74.65      402.9          0.09984   \n",
      "..           ...           ...             ...        ...              ...   \n",
      "148        14.44         15.18           93.97      640.1          0.09970   \n",
      "486        14.64         16.85           94.21      666.0          0.08641   \n",
      "75         16.07         19.65          104.10      817.7          0.09168   \n",
      "249        11.52         14.93           73.87      406.3          0.10130   \n",
      "238        14.22         27.85           92.55      623.9          0.08223   \n",
      "\n",
      "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
      "204           0.10580         0.08005              0.03821         0.1925   \n",
      "70            0.10290         0.10800              0.07951         0.1582   \n",
      "131           0.12230         0.14660              0.08087         0.1931   \n",
      "431           0.13160         0.07741              0.02799         0.1811   \n",
      "540           0.11200         0.06737              0.02594         0.1818   \n",
      "..                ...             ...                  ...            ...   \n",
      "148           0.10210         0.08487              0.05532         0.1724   \n",
      "486           0.06698         0.05192              0.02791         0.1409   \n",
      "75            0.08424         0.09769              0.06638         0.1798   \n",
      "249           0.07808         0.04328              0.02929         0.1883   \n",
      "238           0.10390         0.11030              0.04408         0.1342   \n",
      "\n",
      "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
      "204                 0.06373  ...         14.97          24.64   \n",
      "70                  0.05461  ...         24.86          26.58   \n",
      "131                 0.05796  ...         19.26          26.00   \n",
      "431                 0.07102  ...         12.88          22.91   \n",
      "540                 0.06782  ...         12.26          19.68   \n",
      "..                      ...  ...           ...            ...   \n",
      "148                 0.06081  ...         15.85          19.85   \n",
      "486                 0.05355  ...         16.46          25.44   \n",
      "75                  0.05391  ...         19.77          24.56   \n",
      "249                 0.06168  ...         12.65          21.19   \n",
      "238                 0.06129  ...         15.75          40.54   \n",
      "\n",
      "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
      "204            96.05       677.9            0.1426             0.2378   \n",
      "70            165.90      1866.0            0.1193             0.2336   \n",
      "131           124.90      1156.0            0.1546             0.2394   \n",
      "431            89.61       515.8            0.1450             0.2629   \n",
      "540            78.78       457.8            0.1345             0.2118   \n",
      "..               ...         ...               ...                ...   \n",
      "148           108.60       766.9            0.1316             0.2735   \n",
      "486           106.00       831.0            0.1142             0.2070   \n",
      "75            128.80      1223.0            0.1500             0.2045   \n",
      "249            80.88       491.8            0.1389             0.1582   \n",
      "238           102.50       764.0            0.1081             0.2426   \n",
      "\n",
      "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "204           0.2671               0.10150          0.3014   \n",
      "70            0.2687               0.17890          0.2551   \n",
      "131           0.3791               0.15140          0.2837   \n",
      "431           0.2403               0.07370          0.2556   \n",
      "540           0.1797               0.06918          0.2329   \n",
      "..               ...                   ...             ...   \n",
      "148           0.3103               0.15990          0.2691   \n",
      "486           0.2437               0.07828          0.2455   \n",
      "75            0.2829               0.15200          0.2650   \n",
      "249           0.1804               0.09608          0.2664   \n",
      "238           0.3064               0.08219          0.1890   \n",
      "\n",
      "     fractal_dimension_worst  \n",
      "204                  0.08750  \n",
      "70                   0.06589  \n",
      "131                  0.08019  \n",
      "431                  0.09359  \n",
      "540                  0.08134  \n",
      "..                       ...  \n",
      "148                  0.07683  \n",
      "486                  0.06596  \n",
      "75                   0.06387  \n",
      "249                  0.07809  \n",
      "238                  0.07796  \n",
      "\n",
      "[113 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "x_train = train_set.drop(\"diagnosis\", axis=1)\n",
    "print(x_train)\n",
    "x_test = test_set.drop(\"diagnosis\", axis=1)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "radius_mean               -0.006887\n",
      "texture_mean               0.092006\n",
      "perimeter_mean            -0.001923\n",
      "area_mean                 -0.016484\n",
      "smoothness_mean            0.224187\n",
      "compactness_mean           0.067151\n",
      "concavity_mean            -0.012928\n",
      "concave points_mean        0.073578\n",
      "symmetry_mean              0.013289\n",
      "fractal_dimension_mean     0.037788\n",
      "radius_se                  0.025947\n",
      "texture_se                 0.123422\n",
      "perimeter_se              -0.007070\n",
      "area_se                   -0.008893\n",
      "smoothness_se              0.094418\n",
      "compactness_se            -0.036193\n",
      "concavity_se              -0.142046\n",
      "concave points_se         -0.073795\n",
      "symmetry_se               -0.009917\n",
      "fractal_dimension_se      -0.040196\n",
      "radius_worst              -0.001616\n",
      "texture_worst              0.077232\n",
      "perimeter_worst           -0.011414\n",
      "area_worst                -0.017457\n",
      "smoothness_worst           0.178731\n",
      "compactness_worst          0.048756\n",
      "concavity_worst           -0.061657\n",
      "concave points_worst       0.023976\n",
      "symmetry_worst            -0.033443\n",
      "fractal_dimension_worst    0.023091\n",
      "dtype: float64\n",
      "test std = \n",
      "radius_mean                0.970286\n",
      "texture_mean               1.001488\n",
      "perimeter_mean             0.983889\n",
      "area_mean                  0.936572\n",
      "smoothness_mean            1.037137\n",
      "compactness_mean           1.036944\n",
      "concavity_mean             1.020371\n",
      "concave points_mean        1.093735\n",
      "symmetry_mean              0.992992\n",
      "fractal_dimension_mean     0.893044\n",
      "radius_se                  0.853771\n",
      "texture_se                 1.086549\n",
      "perimeter_se               0.846918\n",
      "area_se                    0.729650\n",
      "smoothness_se              0.906080\n",
      "compactness_se             0.797933\n",
      "concavity_se               0.631754\n",
      "concave points_se          0.900079\n",
      "symmetry_se                1.059335\n",
      "fractal_dimension_se       0.714519\n",
      "radius_worst               0.966865\n",
      "texture_worst              1.001800\n",
      "perimeter_worst            0.984642\n",
      "area_worst                 0.909160\n",
      "smoothness_worst           0.936652\n",
      "compactness_worst          1.078752\n",
      "concavity_worst            0.985787\n",
      "concave points_worst       1.033808\n",
      "symmetry_worst             0.899532\n",
      "fractal_dimension_worst    1.064928\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "#import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).to_numpy().reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).to_numpy().reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1.862968</td>\n",
       "      <td>2.773941</td>\n",
       "      <td>1.795180</td>\n",
       "      <td>2.143314</td>\n",
       "      <td>-0.075698</td>\n",
       "      <td>0.203543</td>\n",
       "      <td>0.601279</td>\n",
       "      <td>1.002020</td>\n",
       "      <td>-0.152741</td>\n",
       "      <td>-0.833785</td>\n",
       "      <td>...</td>\n",
       "      <td>3.336917</td>\n",
       "      <td>3.516585</td>\n",
       "      <td>3.167434</td>\n",
       "      <td>4.403897</td>\n",
       "      <td>0.371113</td>\n",
       "      <td>0.075206</td>\n",
       "      <td>0.332366</td>\n",
       "      <td>0.791132</td>\n",
       "      <td>-0.058623</td>\n",
       "      <td>-0.094562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-1.440918</td>\n",
       "      <td>-0.438193</td>\n",
       "      <td>-1.362689</td>\n",
       "      <td>-1.139308</td>\n",
       "      <td>0.781536</td>\n",
       "      <td>0.719217</td>\n",
       "      <td>2.823755</td>\n",
       "      <td>-0.121348</td>\n",
       "      <td>1.094130</td>\n",
       "      <td>2.460482</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.226353</td>\n",
       "      <td>-0.478037</td>\n",
       "      <td>-1.242402</td>\n",
       "      <td>-0.963709</td>\n",
       "      <td>0.722732</td>\n",
       "      <td>1.187855</td>\n",
       "      <td>4.676639</td>\n",
       "      <td>0.930641</td>\n",
       "      <td>2.099633</td>\n",
       "      <td>1.888605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.964618</td>\n",
       "      <td>1.714099</td>\n",
       "      <td>2.082595</td>\n",
       "      <td>1.839913</td>\n",
       "      <td>1.321356</td>\n",
       "      <td>3.429369</td>\n",
       "      <td>2.013175</td>\n",
       "      <td>2.662741</td>\n",
       "      <td>2.129543</td>\n",
       "      <td>1.560531</td>\n",
       "      <td>...</td>\n",
       "      <td>2.141579</td>\n",
       "      <td>1.287029</td>\n",
       "      <td>2.053839</td>\n",
       "      <td>2.083974</td>\n",
       "      <td>0.761801</td>\n",
       "      <td>3.268981</td>\n",
       "      <td>1.929765</td>\n",
       "      <td>2.698272</td>\n",
       "      <td>1.893329</td>\n",
       "      <td>2.500625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-1.400258</td>\n",
       "      <td>-1.246468</td>\n",
       "      <td>-1.345854</td>\n",
       "      <td>-1.110092</td>\n",
       "      <td>-1.333838</td>\n",
       "      <td>-0.308120</td>\n",
       "      <td>-0.367131</td>\n",
       "      <td>-0.698681</td>\n",
       "      <td>1.932669</td>\n",
       "      <td>0.956396</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.288075</td>\n",
       "      <td>-1.035426</td>\n",
       "      <td>-1.239729</td>\n",
       "      <td>-0.988930</td>\n",
       "      <td>-1.440809</td>\n",
       "      <td>-0.549325</td>\n",
       "      <td>-0.646268</td>\n",
       "      <td>-0.972360</td>\n",
       "      <td>0.598375</td>\n",
       "      <td>0.058162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.983213</td>\n",
       "      <td>1.399641</td>\n",
       "      <td>-0.984122</td>\n",
       "      <td>-0.868215</td>\n",
       "      <td>0.059617</td>\n",
       "      <td>-0.597852</td>\n",
       "      <td>-0.822089</td>\n",
       "      <td>-0.847289</td>\n",
       "      <td>0.313924</td>\n",
       "      <td>0.075888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.827221</td>\n",
       "      <td>1.565723</td>\n",
       "      <td>-0.871798</td>\n",
       "      <td>-0.737418</td>\n",
       "      <td>0.796528</td>\n",
       "      <td>-0.730349</td>\n",
       "      <td>-0.776433</td>\n",
       "      <td>-0.811541</td>\n",
       "      <td>0.799918</td>\n",
       "      <td>-0.134428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-1.480731</td>\n",
       "      <td>-1.064781</td>\n",
       "      <td>-1.362689</td>\n",
       "      <td>-1.157568</td>\n",
       "      <td>0.150307</td>\n",
       "      <td>0.944586</td>\n",
       "      <td>-0.037100</td>\n",
       "      <td>-0.516670</td>\n",
       "      <td>0.332153</td>\n",
       "      <td>3.757635</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.345064</td>\n",
       "      <td>-1.615632</td>\n",
       "      <td>-1.329708</td>\n",
       "      <td>-1.033324</td>\n",
       "      <td>-0.471035</td>\n",
       "      <td>-0.059268</td>\n",
       "      <td>-0.628562</td>\n",
       "      <td>-1.018506</td>\n",
       "      <td>-1.033013</td>\n",
       "      <td>1.377653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.703678</td>\n",
       "      <td>-0.205261</td>\n",
       "      <td>-0.690138</td>\n",
       "      <td>-0.684208</td>\n",
       "      <td>1.328553</td>\n",
       "      <td>-0.037104</td>\n",
       "      <td>-0.230731</td>\n",
       "      <td>-0.355438</td>\n",
       "      <td>-0.036074</td>\n",
       "      <td>0.341152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.644114</td>\n",
       "      <td>0.599256</td>\n",
       "      <td>-0.648188</td>\n",
       "      <td>-0.623408</td>\n",
       "      <td>1.616972</td>\n",
       "      <td>0.085551</td>\n",
       "      <td>0.060071</td>\n",
       "      <td>0.115047</td>\n",
       "      <td>-0.157014</td>\n",
       "      <td>0.398984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.044574</td>\n",
       "      <td>-0.556989</td>\n",
       "      <td>-0.068911</td>\n",
       "      <td>-0.065889</td>\n",
       "      <td>-2.263767</td>\n",
       "      <td>-1.468577</td>\n",
       "      <td>-1.030596</td>\n",
       "      <td>-1.107681</td>\n",
       "      <td>-1.104300</td>\n",
       "      <td>-1.247652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279958</td>\n",
       "      <td>-0.803995</td>\n",
       "      <td>-0.383004</td>\n",
       "      <td>-0.342012</td>\n",
       "      <td>-1.991678</td>\n",
       "      <td>-1.308590</td>\n",
       "      <td>-1.129794</td>\n",
       "      <td>-1.241261</td>\n",
       "      <td>-0.709274</td>\n",
       "      <td>-1.272559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>-0.042957</td>\n",
       "      <td>0.095221</td>\n",
       "      <td>-0.035243</td>\n",
       "      <td>-0.158876</td>\n",
       "      <td>0.738350</td>\n",
       "      <td>0.184444</td>\n",
       "      <td>0.297469</td>\n",
       "      <td>0.427842</td>\n",
       "      <td>-0.517323</td>\n",
       "      <td>0.374483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158264</td>\n",
       "      <td>0.850244</td>\n",
       "      <td>0.194878</td>\n",
       "      <td>-0.022958</td>\n",
       "      <td>1.291399</td>\n",
       "      <td>0.672584</td>\n",
       "      <td>0.632417</td>\n",
       "      <td>1.048687</td>\n",
       "      <td>0.434919</td>\n",
       "      <td>1.214822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.551204</td>\n",
       "      <td>0.304860</td>\n",
       "      <td>-0.605966</td>\n",
       "      <td>-0.554420</td>\n",
       "      <td>-1.123668</td>\n",
       "      <td>-1.208257</td>\n",
       "      <td>-0.821585</td>\n",
       "      <td>-0.806520</td>\n",
       "      <td>-0.262115</td>\n",
       "      <td>-0.829618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.602967</td>\n",
       "      <td>1.182722</td>\n",
       "      <td>-0.675805</td>\n",
       "      <td>-0.578323</td>\n",
       "      <td>-0.835677</td>\n",
       "      <td>-1.061234</td>\n",
       "      <td>-0.766862</td>\n",
       "      <td>-0.613008</td>\n",
       "      <td>-0.334753</td>\n",
       "      <td>-0.846953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "265     1.862968      2.773941        1.795180   2.143314        -0.075698   \n",
       "68     -1.440918     -0.438193       -1.362689  -1.139308         0.781536   \n",
       "181     1.964618      1.714099        2.082595   1.839913         1.321356   \n",
       "63     -1.400258     -1.246468       -1.345854  -1.110092        -1.333838   \n",
       "248    -0.983213      1.399641       -0.984122  -0.868215         0.059617   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "71     -1.480731     -1.064781       -1.362689  -1.157568         0.150307   \n",
       "106    -0.703678     -0.205261       -0.690138  -0.684208         1.328553   \n",
       "270     0.044574     -0.556989       -0.068911  -0.065889        -2.263767   \n",
       "435    -0.042957      0.095221       -0.035243  -0.158876         0.738350   \n",
       "102    -0.551204      0.304860       -0.605966  -0.554420        -1.123668   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "265          0.203543        0.601279             1.002020      -0.152741   \n",
       "68           0.719217        2.823755            -0.121348       1.094130   \n",
       "181          3.429369        2.013175             2.662741       2.129543   \n",
       "63          -0.308120       -0.367131            -0.698681       1.932669   \n",
       "248         -0.597852       -0.822089            -0.847289       0.313924   \n",
       "..                ...             ...                  ...            ...   \n",
       "71           0.944586       -0.037100            -0.516670       0.332153   \n",
       "106         -0.037104       -0.230731            -0.355438      -0.036074   \n",
       "270         -1.468577       -1.030596            -1.107681      -1.104300   \n",
       "435          0.184444        0.297469             0.427842      -0.517323   \n",
       "102         -1.208257       -0.821585            -0.806520      -0.262115   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "265               -0.833785  ...      3.336917       3.516585   \n",
       "68                 2.460482  ...     -1.226353      -0.478037   \n",
       "181                1.560531  ...      2.141579       1.287029   \n",
       "63                 0.956396  ...     -1.288075      -1.035426   \n",
       "248                0.075888  ...     -0.827221       1.565723   \n",
       "..                      ...  ...           ...            ...   \n",
       "71                 3.757635  ...     -1.345064      -1.615632   \n",
       "106                0.341152  ...     -0.644114       0.599256   \n",
       "270               -1.247652  ...     -0.279958      -0.803995   \n",
       "435                0.374483  ...      0.158264       0.850244   \n",
       "102               -0.829618  ...     -0.602967       1.182722   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "265         3.167434    4.403897          0.371113           0.075206   \n",
       "68         -1.242402   -0.963709          0.722732           1.187855   \n",
       "181         2.053839    2.083974          0.761801           3.268981   \n",
       "63         -1.239729   -0.988930         -1.440809          -0.549325   \n",
       "248        -0.871798   -0.737418          0.796528          -0.730349   \n",
       "..               ...         ...               ...                ...   \n",
       "71         -1.329708   -1.033324         -0.471035          -0.059268   \n",
       "106        -0.648188   -0.623408          1.616972           0.085551   \n",
       "270        -0.383004   -0.342012         -1.991678          -1.308590   \n",
       "435         0.194878   -0.022958          1.291399           0.672584   \n",
       "102        -0.675805   -0.578323         -0.835677          -1.061234   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "265         0.332366              0.791132       -0.058623   \n",
       "68          4.676639              0.930641        2.099633   \n",
       "181         1.929765              2.698272        1.893329   \n",
       "63         -0.646268             -0.972360        0.598375   \n",
       "248        -0.776433             -0.811541        0.799918   \n",
       "..               ...                   ...             ...   \n",
       "71         -0.628562             -1.018506       -1.033013   \n",
       "106         0.060071              0.115047       -0.157014   \n",
       "270        -1.129794             -1.241261       -0.709274   \n",
       "435         0.632417              1.048687        0.434919   \n",
       "102        -0.766862             -0.613008       -0.334753   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "265                -0.094562  \n",
       "68                  1.888605  \n",
       "181                 2.500625  \n",
       "63                  0.058162  \n",
       "248                -0.134428  \n",
       "..                       ...  \n",
       "71                  1.377653  \n",
       "106                 0.398984  \n",
       "270                -1.272559  \n",
       "435                 1.214822  \n",
       "102                -0.846953  \n",
       "\n",
       "[456 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Logistic Regression Model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "When $\\lambda = 0$, the model is a regular logistric regression and when $\\lambda > 0$, it essentially becomes a regularized logistric regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value, or loss\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     objective function value, or loss (scalar)\n",
    "\n",
    "def objective(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    y = y.reshape(n,1) # n-by-1 matrix\n",
    "    y_x = y*x # n-by-d matrix\n",
    "    y_x_w = numpy.dot(y_x, w) # n-by-1 matrix\n",
    "    sum1 = numpy.exp(-y_x_w) # n-by-1 matrix\n",
    "    sum2 = numpy.log(1 + sum1) # n-by-1 matrix\n",
    "    sum3 = numpy.mean(sum2) # scalar\n",
    "    regularization_param = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    loss = sum3 +regularization_param # scalar\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     g: gradient: d-by-1 matrix\n",
    "\n",
    "def gradient(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    y = y.reshape(n,1) # n-by-1 matrix\n",
    "    y_x = y*x # n-by-d matrix\n",
    "    y_x_w = numpy.dot(y_x, w) # n-by-1 matrix\n",
    "    sum1 = numpy.exp(y_x_w) # n-by-1 matrix\n",
    "    sum2 = numpy.divide(y_x, 1+sum1) # n-by-d matrix\n",
    "    sum3 = -numpy.mean(sum2, axis=0).reshape(d, 1) # d-by-1 matrix\n",
    "    grad = sum3 + lam * w # d-by-1 matrix\n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    n, d = x.shape\n",
    "    numpy.random.seed(42) # using a random seed to obtain consistent values on each run\n",
    "    w = numpy.random.randn(d,1) # Random initilization\n",
    "    gd_loss_record = [] # store the objective values\n",
    "    \n",
    "    \n",
    "    for i in range(max_epoch):\n",
    "        loss = objective(w, x, y, lam)\n",
    "        gd_loss_record.append(loss)\n",
    "        grad = gradient(w, x, y, lam)\n",
    "        w = w - learning_rate*grad\n",
    "        print(\"Epoch\", i+1,\"/\",max_epoch,\"------>\",loss)\n",
    "        \n",
    "    print(\"gd_loss_record\",gd_loss_record)\n",
    "    print(\"length of gd_loss_record ->\",len(gd_loss_record))\n",
    "    return w,gd_loss_record\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 1.6900504384455077\n",
      "Epoch 2 / 100 ------> 1.4753419148813591\n",
      "Epoch 3 / 100 ------> 1.2936066134917032\n",
      "Epoch 4 / 100 ------> 1.1403470774396498\n",
      "Epoch 5 / 100 ------> 1.0109416270640443\n",
      "Epoch 6 / 100 ------> 0.9020333304891125\n",
      "Epoch 7 / 100 ------> 0.8109896917804706\n",
      "Epoch 8 / 100 ------> 0.7350407709529481\n",
      "Epoch 9 / 100 ------> 0.6713078401586887\n",
      "Epoch 10 / 100 ------> 0.6174569075009695\n",
      "Epoch 11 / 100 ------> 0.5717362923838056\n",
      "Epoch 12 / 100 ------> 0.5327576095540021\n",
      "Epoch 13 / 100 ------> 0.49938844667495574\n",
      "Epoch 14 / 100 ------> 0.4707201176320212\n",
      "Epoch 15 / 100 ------> 0.44602340526490064\n",
      "Epoch 16 / 100 ------> 0.424684517782585\n",
      "Epoch 17 / 100 ------> 0.40616134390981273\n",
      "Epoch 18 / 100 ------> 0.38997732139938973\n",
      "Epoch 19 / 100 ------> 0.3757288075940639\n",
      "Epoch 20 / 100 ------> 0.3630840452143259\n",
      "Epoch 21 / 100 ------> 0.3517739809535371\n",
      "Epoch 22 / 100 ------> 0.3415815365552621\n",
      "Epoch 23 / 100 ------> 0.33233211486636755\n",
      "Epoch 24 / 100 ------> 0.32388547496456943\n",
      "Epoch 25 / 100 ------> 0.3161287636001828\n",
      "Epoch 26 / 100 ------> 0.3089706710136133\n",
      "Epoch 27 / 100 ------> 0.3023367002760546\n",
      "Epoch 28 / 100 ------> 0.2961654563074363\n",
      "Epoch 29 / 100 ------> 0.2904057949129597\n",
      "Epoch 30 / 100 ------> 0.28501465491215566\n",
      "Epoch 31 / 100 ------> 0.2799554126200562\n",
      "Epoch 32 / 100 ------> 0.27519662743852497\n",
      "Epoch 33 / 100 ------> 0.27071107766279023\n",
      "Epoch 34 / 100 ------> 0.26647501150215175\n",
      "Epoch 35 / 100 ------> 0.26246755852253195\n",
      "Epoch 36 / 100 ------> 0.2586702617614438\n",
      "Epoch 37 / 100 ------> 0.2550667016822347\n",
      "Epoch 38 / 100 ------> 0.25164219095411783\n",
      "Epoch 39 / 100 ------> 0.2483835246146557\n",
      "Epoch 40 / 100 ------> 0.24527877413416652\n",
      "Epoch 41 / 100 ------> 0.24231711672483533\n",
      "Epoch 42 / 100 ------> 0.2394886932558335\n",
      "Epoch 43 / 100 ------> 0.23678448958756823\n",
      "Epoch 44 / 100 ------> 0.23419623719276542\n",
      "Epoch 45 / 100 ------> 0.231716329710261\n",
      "Epoch 46 / 100 ------> 0.22933775266513043\n",
      "Epoch 47 / 100 ------> 0.2270540240466554\n",
      "Epoch 48 / 100 ------> 0.2248591438052785\n",
      "Epoch 49 / 100 ------> 0.22274755063874177\n",
      "Epoch 50 / 100 ------> 0.22071408470334394\n",
      "Epoch 51 / 100 ------> 0.21875395511873025\n",
      "Epoch 52 / 100 ------> 0.21686271133911755\n",
      "Epoch 53 / 100 ------> 0.2150362176428356\n",
      "Epoch 54 / 100 ------> 0.2132706301466885\n",
      "Epoch 55 / 100 ------> 0.21156237588275043\n",
      "Epoch 56 / 100 ------> 0.20990813358387692\n",
      "Epoch 57 / 100 ------> 0.20830481591201938\n",
      "Epoch 58 / 100 ------> 0.20674955293242422\n",
      "Epoch 59 / 100 ------> 0.2052396766894144\n",
      "Epoch 60 / 100 ------> 0.20377270677827533\n",
      "Epoch 61 / 100 ------> 0.2023463368353957\n",
      "Epoch 62 / 100 ------> 0.200958421887652\n",
      "Epoch 63 / 100 ------> 0.19960696651423915\n",
      "Epoch 64 / 100 ------> 0.198290113781547\n",
      "Epoch 65 / 100 ------> 0.19700613491576\n",
      "Epoch 66 / 100 ------> 0.19575341967976737\n",
      "Epoch 67 / 100 ------> 0.1945304674215902\n",
      "Epoch 68 / 100 ------> 0.19333587876147929\n",
      "Epoch 69 / 100 ------> 0.1921683478845489\n",
      "Epoch 70 / 100 ------> 0.19102665540555153\n",
      "Epoch 71 / 100 ------> 0.1899096617723372\n",
      "Epoch 72 / 100 ------> 0.18881630117474496\n",
      "Epoch 73 / 100 ------> 0.18774557592617294\n",
      "Epoch 74 / 100 ------> 0.18669655128585097\n",
      "Epoch 75 / 100 ------> 0.18566835069086127\n",
      "Epoch 76 / 100 ------> 0.18466015136817435\n",
      "Epoch 77 / 100 ------> 0.18367118029833998\n",
      "Epoch 78 / 100 ------> 0.18270071050395548\n",
      "Epoch 79 / 100 ------> 0.18174805763757632\n",
      "Epoch 80 / 100 ------> 0.18081257684530783\n",
      "Epoch 81 / 100 ------> 0.1798936598838879\n",
      "Epoch 82 / 100 ------> 0.17899073247061412\n",
      "Epoch 83 / 100 ------> 0.17810325184697012\n",
      "Epoch 84 / 100 ------> 0.177230704538244\n",
      "Epoch 85 / 100 ------> 0.17637260429280532\n",
      "Epoch 86 / 100 ------> 0.17552849018600192\n",
      "Epoch 87 / 100 ------> 0.17469792487485686\n",
      "Epoch 88 / 100 ------> 0.17388049299088207\n",
      "Epoch 89 / 100 ------> 0.17307579965938513\n",
      "Epoch 90 / 100 ------> 0.17228346913462517\n",
      "Epoch 91 / 100 ------> 0.17150314354108076\n",
      "Epoch 92 / 100 ------> 0.17073448171192682\n",
      "Epoch 93 / 100 ------> 0.16997715811658487\n",
      "Epoch 94 / 100 ------> 0.1692308618699141\n",
      "Epoch 95 / 100 ------> 0.16849529581625614\n",
      "Epoch 96 / 100 ------> 0.16777017568213462\n",
      "Epoch 97 / 100 ------> 0.16705522929194982\n",
      "Epoch 98 / 100 ------> 0.16635019584150026\n",
      "Epoch 99 / 100 ------> 0.16565482522461059\n",
      "Epoch 100 / 100 ------> 0.1649688774085546\n",
      "gd_loss_record [1.6900504384455077, 1.4753419148813591, 1.2936066134917032, 1.1403470774396498, 1.0109416270640443, 0.9020333304891125, 0.8109896917804706, 0.7350407709529481, 0.6713078401586887, 0.6174569075009695, 0.5717362923838056, 0.5327576095540021, 0.49938844667495574, 0.4707201176320212, 0.44602340526490064, 0.424684517782585, 0.40616134390981273, 0.38997732139938973, 0.3757288075940639, 0.3630840452143259, 0.3517739809535371, 0.3415815365552621, 0.33233211486636755, 0.32388547496456943, 0.3161287636001828, 0.3089706710136133, 0.3023367002760546, 0.2961654563074363, 0.2904057949129597, 0.28501465491215566, 0.2799554126200562, 0.27519662743852497, 0.27071107766279023, 0.26647501150215175, 0.26246755852253195, 0.2586702617614438, 0.2550667016822347, 0.25164219095411783, 0.2483835246146557, 0.24527877413416652, 0.24231711672483533, 0.2394886932558335, 0.23678448958756823, 0.23419623719276542, 0.231716329710261, 0.22933775266513043, 0.2270540240466554, 0.2248591438052785, 0.22274755063874177, 0.22071408470334394, 0.21875395511873025, 0.21686271133911755, 0.2150362176428356, 0.2132706301466885, 0.21156237588275043, 0.20990813358387692, 0.20830481591201938, 0.20674955293242422, 0.2052396766894144, 0.20377270677827533, 0.2023463368353957, 0.200958421887652, 0.19960696651423915, 0.198290113781547, 0.19700613491576, 0.19575341967976737, 0.1945304674215902, 0.19333587876147929, 0.1921683478845489, 0.19102665540555153, 0.1899096617723372, 0.18881630117474496, 0.18774557592617294, 0.18669655128585097, 0.18566835069086127, 0.18466015136817435, 0.18367118029833998, 0.18270071050395548, 0.18174805763757632, 0.18081257684530783, 0.1798936598838879, 0.17899073247061412, 0.17810325184697012, 0.177230704538244, 0.17637260429280532, 0.17552849018600192, 0.17469792487485686, 0.17388049299088207, 0.17307579965938513, 0.17228346913462517, 0.17150314354108076, 0.17073448171192682, 0.16997715811658487, 0.1692308618699141, 0.16849529581625614, 0.16777017568213462, 0.16705522929194982, 0.16635019584150026, 0.16565482522461059, 0.1649688774085546]\n",
      "length of gd_loss_record -> 100\n",
      "[[ 0.56539935]\n",
      " [ 0.50581484]\n",
      " [ 0.73870905]\n",
      " [ 1.67957647]\n",
      " [ 0.14934589]\n",
      " [ 0.05356036]\n",
      " [ 2.05354894]\n",
      " [ 1.15417034]\n",
      " [ 0.02478618]\n",
      " [ 0.85186532]\n",
      " [ 0.13950439]\n",
      " [ 0.0980295 ]\n",
      " [ 0.78200226]\n",
      " [-1.45731458]\n",
      " [-0.9634595 ]\n",
      " [-0.15217489]\n",
      " [-0.66272061]\n",
      " [ 0.68610536]\n",
      " [-0.31578217]\n",
      " [-1.03654241]\n",
      " [ 1.69269021]\n",
      " [ 0.47031019]\n",
      " [ 0.30012126]\n",
      " [-1.1366432 ]\n",
      " [ 0.08766772]\n",
      " [ 0.48036536]\n",
      " [-0.70515957]\n",
      " [ 0.70805257]\n",
      " [ 0.00433801]\n",
      " [ 0.16846704]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "gd_weights,gd_loss_record = gradient_descent(x_train_array,y_train_array,0,0.1,w,100)\n",
    "print(gd_weights,gd_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.86296828,  2.77394061,  1.79517976, ...,  0.79113154,\n",
       "        -0.05862331, -0.09456232],\n",
       "       [-1.44091791, -0.43819334, -1.36268878, ...,  0.93064101,\n",
       "         2.09963316,  1.88860526],\n",
       "       [ 1.96461763,  1.71409946,  2.08259466, ...,  2.69827196,\n",
       "         1.89332923,  2.5006247 ],\n",
       "       ...,\n",
       "       [ 0.04457438, -0.55698872, -0.06891114, ..., -1.24126078,\n",
       "        -0.70927415, -1.27255938],\n",
       "       [-0.042957  ,  0.09522122, -0.03524254, ...,  1.04868748,\n",
       "         0.43491916,  1.21482239],\n",
       "       [-0.55120375,  0.30486013, -0.60596641, ..., -0.61300825,\n",
       "        -0.33475318, -0.8469532 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 185.8323990196105\n",
      "Epoch 2 / 100 ------> 134.31756501643886\n",
      "Epoch 3 / 100 ------> 97.10932814729368\n",
      "Epoch 4 / 100 ------> 70.2381673576664\n",
      "Epoch 5 / 100 ------> 50.835816328300886\n",
      "Epoch 6 / 100 ------> 36.82961964354647\n",
      "Epoch 7 / 100 ------> 26.721733020338398\n",
      "Epoch 8 / 100 ------> 19.42963512210921\n",
      "Epoch 9 / 100 ------> 14.17094055244912\n",
      "Epoch 10 / 100 ------> 10.38020899050756\n",
      "Epoch 11 / 100 ------> 7.648847629491139\n",
      "Epoch 12 / 100 ------> 5.681674330968635\n",
      "Epoch 13 / 100 ------> 4.26550598936431\n",
      "Epoch 14 / 100 ------> 3.246436956411373\n",
      "Epoch 15 / 100 ------> 2.513404151822067\n",
      "Epoch 16 / 100 ------> 1.9863011059935993\n",
      "Epoch 17 / 100 ------> 1.607382308002041\n",
      "Epoch 18 / 100 ------> 1.3350469845548476\n",
      "Epoch 19 / 100 ------> 1.139343991404862\n",
      "Epoch 20 / 100 ------> 0.9987222175755037\n",
      "Epoch 21 / 100 ------> 0.8976827050505578\n",
      "Epoch 22 / 100 ------> 0.8250836580947722\n",
      "Epoch 23 / 100 ------> 0.7729179943348188\n",
      "Epoch 24 / 100 ------> 0.7354325947097038\n",
      "Epoch 25 / 100 ------> 0.7084943273104791\n",
      "Epoch 26 / 100 ------> 0.689134049472051\n",
      "Epoch 27 / 100 ------> 0.6752188122207717\n",
      "Epoch 28 / 100 ------> 0.6652163182797665\n",
      "Epoch 29 / 100 ------> 0.6580257141103697\n",
      "Epoch 30 / 100 ------> 0.6528560529647918\n",
      "Epoch 31 / 100 ------> 0.6491390045264624\n",
      "Epoch 32 / 100 ------> 0.6464661618022683\n",
      "Epoch 33 / 100 ------> 0.6445440127462226\n",
      "Epoch 34 / 100 ------> 0.6431615974656794\n",
      "Epoch 35 / 100 ------> 0.6421672754343507\n",
      "Epoch 36 / 100 ------> 0.6414520352515899\n",
      "Epoch 37 / 100 ------> 0.6409375033927726\n",
      "Epoch 38 / 100 ------> 0.6405673281523714\n",
      "Epoch 39 / 100 ------> 0.6403009881454961\n",
      "Epoch 40 / 100 ------> 0.6401093426534055\n",
      "Epoch 41 / 100 ------> 0.6399714334653629\n",
      "Epoch 42 / 100 ------> 0.6398721859988907\n",
      "Epoch 43 / 100 ------> 0.6398007566736059\n",
      "Epoch 44 / 100 ------> 0.6397493447529038\n",
      "Epoch 45 / 100 ------> 0.6397123380358626\n",
      "Epoch 46 / 100 ------> 0.6396856985375803\n",
      "Epoch 47 / 100 ------> 0.6396665207019588\n",
      "Epoch 48 / 100 ------> 0.6396527136632023\n",
      "Epoch 49 / 100 ------> 0.6396427727050593\n",
      "Epoch 50 / 100 ------> 0.6396356148638751\n",
      "Epoch 51 / 100 ------> 0.6396304606627666\n",
      "Epoch 52 / 100 ------> 0.6396267490253666\n",
      "Epoch 53 / 100 ------> 0.6396240760558236\n",
      "Epoch 54 / 100 ------> 0.6396221509873417\n",
      "Epoch 55 / 100 ------> 0.6396207644821658\n",
      "Epoch 56 / 100 ------> 0.6396197658181737\n",
      "Epoch 57 / 100 ------> 0.6396190464696924\n",
      "Epoch 58 / 100 ------> 0.6396185282895251\n",
      "Epoch 59 / 100 ------> 0.6396181550022008\n",
      "Epoch 60 / 100 ------> 0.6396178860802848\n",
      "Epoch 61 / 100 ------> 0.6396176923358856\n",
      "Epoch 62 / 100 ------> 0.6396175527467414\n",
      "Epoch 63 / 100 ------> 0.6396174521710116\n",
      "Epoch 64 / 100 ------> 0.6396173797018323\n",
      "Epoch 65 / 100 ------> 0.6396173274824594\n",
      "Epoch 66 / 100 ------> 0.6396172898530254\n",
      "Epoch 67 / 100 ------> 0.6396172627360641\n",
      "Epoch 68 / 100 ------> 0.6396172431939638\n",
      "Epoch 69 / 100 ------> 0.6396172291102253\n",
      "Epoch 70 / 100 ------> 0.6396172189598817\n",
      "Epoch 71 / 100 ------> 0.6396172116441251\n",
      "Epoch 72 / 100 ------> 0.6396172063711819\n",
      "Epoch 73 / 100 ------> 0.6396172025704967\n",
      "Epoch 74 / 100 ------> 0.6396171998309084\n",
      "Epoch 75 / 100 ------> 0.6396171978561087\n",
      "Epoch 76 / 100 ------> 0.6396171964325524\n",
      "Epoch 77 / 100 ------> 0.6396171954063339\n",
      "Epoch 78 / 100 ------> 0.639617194666527\n",
      "Epoch 79 / 100 ------> 0.63961719413318\n",
      "Epoch 80 / 100 ------> 0.6396171937486643\n",
      "Epoch 81 / 100 ------> 0.6396171934714405\n",
      "Epoch 82 / 100 ------> 0.6396171932715653\n",
      "Epoch 83 / 100 ------> 0.6396171931274535\n",
      "Epoch 84 / 100 ------> 0.6396171930235449\n",
      "Epoch 85 / 100 ------> 0.6396171929486221\n",
      "Epoch 86 / 100 ------> 0.6396171928945978\n",
      "Epoch 87 / 100 ------> 0.6396171928556418\n",
      "Epoch 88 / 100 ------> 0.6396171928275508\n",
      "Epoch 89 / 100 ------> 0.6396171928072939\n",
      "Epoch 90 / 100 ------> 0.6396171927926858\n",
      "Epoch 91 / 100 ------> 0.6396171927821512\n",
      "Epoch 92 / 100 ------> 0.639617192774554\n",
      "Epoch 93 / 100 ------> 0.6396171927690752\n",
      "Epoch 94 / 100 ------> 0.6396171927651239\n",
      "Epoch 95 / 100 ------> 0.639617192762274\n",
      "Epoch 96 / 100 ------> 0.6396171927602187\n",
      "Epoch 97 / 100 ------> 0.6396171927587363\n",
      "Epoch 98 / 100 ------> 0.6396171927576672\n",
      "Epoch 99 / 100 ------> 0.6396171927568959\n",
      "Epoch 100 / 100 ------> 0.6396171927563398\n",
      "gd_loss_record [185.8323990196105, 134.31756501643886, 97.10932814729368, 70.2381673576664, 50.835816328300886, 36.82961964354647, 26.721733020338398, 19.42963512210921, 14.17094055244912, 10.38020899050756, 7.648847629491139, 5.681674330968635, 4.26550598936431, 3.246436956411373, 2.513404151822067, 1.9863011059935993, 1.607382308002041, 1.3350469845548476, 1.139343991404862, 0.9987222175755037, 0.8976827050505578, 0.8250836580947722, 0.7729179943348188, 0.7354325947097038, 0.7084943273104791, 0.689134049472051, 0.6752188122207717, 0.6652163182797665, 0.6580257141103697, 0.6528560529647918, 0.6491390045264624, 0.6464661618022683, 0.6445440127462226, 0.6431615974656794, 0.6421672754343507, 0.6414520352515899, 0.6409375033927726, 0.6405673281523714, 0.6403009881454961, 0.6401093426534055, 0.6399714334653629, 0.6398721859988907, 0.6398007566736059, 0.6397493447529038, 0.6397123380358626, 0.6396856985375803, 0.6396665207019588, 0.6396527136632023, 0.6396427727050593, 0.6396356148638751, 0.6396304606627666, 0.6396267490253666, 0.6396240760558236, 0.6396221509873417, 0.6396207644821658, 0.6396197658181737, 0.6396190464696924, 0.6396185282895251, 0.6396181550022008, 0.6396178860802848, 0.6396176923358856, 0.6396175527467414, 0.6396174521710116, 0.6396173797018323, 0.6396173274824594, 0.6396172898530254, 0.6396172627360641, 0.6396172431939638, 0.6396172291102253, 0.6396172189598817, 0.6396172116441251, 0.6396172063711819, 0.6396172025704967, 0.6396171998309084, 0.6396171978561087, 0.6396171964325524, 0.6396171954063339, 0.639617194666527, 0.63961719413318, 0.6396171937486643, 0.6396171934714405, 0.6396171932715653, 0.6396171931274535, 0.6396171930235449, 0.6396171929486221, 0.6396171928945978, 0.6396171928556418, 0.6396171928275508, 0.6396171928072939, 0.6396171927926858, 0.6396171927821512, 0.639617192774554, 0.6396171927690752, 0.6396171927651239, 0.639617192762274, 0.6396171927602187, 0.6396171927587363, 0.6396171927576672, 0.6396171927568959, 0.6396171927563398]\n",
      "length of gd_loss_record -> 100\n",
      "[[ 0.01944558]\n",
      " [ 0.01170014]\n",
      " [ 0.01976776]\n",
      " [ 0.01870394]\n",
      " [ 0.00986355]\n",
      " [ 0.01537549]\n",
      " [ 0.01801967]\n",
      " [ 0.02086044]\n",
      " [ 0.00906931]\n",
      " [-0.00108689]\n",
      " [ 0.01426201]\n",
      " [-0.00020905]\n",
      " [ 0.01386054]\n",
      " [ 0.01352503]\n",
      " [-0.00189189]\n",
      " [ 0.00593061]\n",
      " [ 0.00495592]\n",
      " [ 0.00962052]\n",
      " [-0.0003156 ]\n",
      " [ 0.00024061]\n",
      " [ 0.02075526]\n",
      " [ 0.01321735]\n",
      " [ 0.02091481]\n",
      " [ 0.01933232]\n",
      " [ 0.01180827]\n",
      " [ 0.01567521]\n",
      " [ 0.01735218]\n",
      " [ 0.02131943]\n",
      " [ 0.01213942]\n",
      " [ 0.00821072]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "gd_weights_reg,gd_loss_record_reg = gradient_descent(x_train_array,y_train_array,15,0.01,w,100)\n",
    "print(gd_weights_reg,gd_weights_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: 1-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    \n",
    "    xi = xi.reshape(1,30)\n",
    "    y_x = yi * xi # 1-by-d matrix\n",
    "\n",
    "    y_x_w = float(numpy.dot(y_x, w)) # scalar\n",
    "    \n",
    "    # calculate objective function Q_i\n",
    "    loss = numpy.log(1 + numpy.exp(-y_x_w)) # scalar\n",
    "    regularization_param = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    loss = loss + regularization_param\n",
    "    \n",
    "    # calculate stochastic gradient\n",
    "    grad = -y_x.T / (1 + numpy.exp(y_x_w)) # d-by-1 matrix\n",
    "    grad = grad + lam * w # d-by-1 matrix\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.37787111]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = numpy.random.randn(1,5)\n",
    "b = numpy.random.randn(5,1)\n",
    "c = numpy.dot(a,b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples.\n",
    "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     \n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    n, d = x.shape\n",
    "    numpy.random.seed(42)\n",
    "    w = numpy.random.randn(d,1) # Random initilization\n",
    "\n",
    "    sgd_loss_record = [] # store the objective values\n",
    "    \n",
    "    for i in range(max_epoch):\n",
    "        # randomly shuffle the samples\n",
    "        random_indices = numpy.random.permutation(n)\n",
    "        x_indices = x[random_indices, :]\n",
    "        y_indices = y[random_indices]\n",
    "        \n",
    "        loss = 0 # accumulate the objective values\n",
    "        for j in range(n):\n",
    "            xi = x_indices[j, :] # 1-by-d matrix\n",
    "            yi = (y_indices[j]) # scalar\n",
    "            initial_loss, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            loss = loss +initial_loss\n",
    "            w = w - learning_rate*g\n",
    "        \n",
    "        #stepsize *= 0.9 # decrease step size\n",
    "        loss = loss/n\n",
    "        sgd_loss_record.append(loss)\n",
    "        print(\"Epoch\", i+1,\"/\",max_epoch,\"------>\",loss)\n",
    "    print(\"sgd_loss_record\",sgd_loss_record)\n",
    "    print(\"length of sgd_loss_record ->\",len(sgd_loss_record))\n",
    "    return w, sgd_loss_record\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 0.2020103054530172\n",
      "Epoch 2 / 100 ------> 0.08895907286582184\n",
      "Epoch 3 / 100 ------> 0.07594480001833694\n",
      "Epoch 4 / 100 ------> 0.073295522156909\n",
      "Epoch 5 / 100 ------> 0.06370352294970562\n",
      "Epoch 6 / 100 ------> 0.05919164928574551\n",
      "Epoch 7 / 100 ------> 0.05913900437219594\n",
      "Epoch 8 / 100 ------> 0.05826423717421941\n",
      "Epoch 9 / 100 ------> 0.05858020484321446\n",
      "Epoch 10 / 100 ------> 0.06087374696051452\n",
      "Epoch 11 / 100 ------> 0.06293771620803511\n",
      "Epoch 12 / 100 ------> 0.05445698157917128\n",
      "Epoch 13 / 100 ------> 0.053315121661762645\n",
      "Epoch 14 / 100 ------> 0.057631994248346335\n",
      "Epoch 15 / 100 ------> 0.050641841539062996\n",
      "Epoch 16 / 100 ------> 0.051386533592840206\n",
      "Epoch 17 / 100 ------> 0.05216469953002802\n",
      "Epoch 18 / 100 ------> 0.061976315120602074\n",
      "Epoch 19 / 100 ------> 0.0597619170532856\n",
      "Epoch 20 / 100 ------> 0.051821025717605225\n",
      "Epoch 21 / 100 ------> 0.04936917098741923\n",
      "Epoch 22 / 100 ------> 0.052384492101023826\n",
      "Epoch 23 / 100 ------> 0.0486159093712775\n",
      "Epoch 24 / 100 ------> 0.047557879921313995\n",
      "Epoch 25 / 100 ------> 0.05013443276455654\n",
      "Epoch 26 / 100 ------> 0.047832166433937756\n",
      "Epoch 27 / 100 ------> 0.04678844014674751\n",
      "Epoch 28 / 100 ------> 0.05526679518458867\n",
      "Epoch 29 / 100 ------> 0.04665934199293531\n",
      "Epoch 30 / 100 ------> 0.05875595101750927\n",
      "Epoch 31 / 100 ------> 0.053739202473935294\n",
      "Epoch 32 / 100 ------> 0.04700221674523138\n",
      "Epoch 33 / 100 ------> 0.04789771247481931\n",
      "Epoch 34 / 100 ------> 0.046707574937090386\n",
      "Epoch 35 / 100 ------> 0.04949347124379247\n",
      "Epoch 36 / 100 ------> 0.047638623952877386\n",
      "Epoch 37 / 100 ------> 0.04511177974094944\n",
      "Epoch 38 / 100 ------> 0.044762422681394\n",
      "Epoch 39 / 100 ------> 0.04940741826842766\n",
      "Epoch 40 / 100 ------> 0.05088644880491742\n",
      "Epoch 41 / 100 ------> 0.04864876308877365\n",
      "Epoch 42 / 100 ------> 0.045846769151004\n",
      "Epoch 43 / 100 ------> 0.04500879838177982\n",
      "Epoch 44 / 100 ------> 0.04570218547748727\n",
      "Epoch 45 / 100 ------> 0.053666773514007046\n",
      "Epoch 46 / 100 ------> 0.04587913369578108\n",
      "Epoch 47 / 100 ------> 0.046720351713432716\n",
      "Epoch 48 / 100 ------> 0.04828424653373514\n",
      "Epoch 49 / 100 ------> 0.04461506718341773\n",
      "Epoch 50 / 100 ------> 0.0456867922150884\n",
      "Epoch 51 / 100 ------> 0.04610042790411315\n",
      "Epoch 52 / 100 ------> 0.043305703291858\n",
      "Epoch 53 / 100 ------> 0.042520795078451495\n",
      "Epoch 54 / 100 ------> 0.0423521024494201\n",
      "Epoch 55 / 100 ------> 0.04466620595143589\n",
      "Epoch 56 / 100 ------> 0.04307292125050517\n",
      "Epoch 57 / 100 ------> 0.04578055050088459\n",
      "Epoch 58 / 100 ------> 0.046097592343684206\n",
      "Epoch 59 / 100 ------> 0.04911728882218681\n",
      "Epoch 60 / 100 ------> 0.04068193447960834\n",
      "Epoch 61 / 100 ------> 0.042739378698825346\n",
      "Epoch 62 / 100 ------> 0.042846036500435315\n",
      "Epoch 63 / 100 ------> 0.04170172784807263\n",
      "Epoch 64 / 100 ------> 0.045404399444226556\n",
      "Epoch 65 / 100 ------> 0.04386653171948104\n",
      "Epoch 66 / 100 ------> 0.04479008576417744\n",
      "Epoch 67 / 100 ------> 0.040392376778858076\n",
      "Epoch 68 / 100 ------> 0.04208044593958689\n",
      "Epoch 69 / 100 ------> 0.042176149420118755\n",
      "Epoch 70 / 100 ------> 0.04094329298505463\n",
      "Epoch 71 / 100 ------> 0.04012747807895058\n",
      "Epoch 72 / 100 ------> 0.05099796924227082\n",
      "Epoch 73 / 100 ------> 0.042674727813698785\n",
      "Epoch 74 / 100 ------> 0.04171793033443865\n",
      "Epoch 75 / 100 ------> 0.041850257652460414\n",
      "Epoch 76 / 100 ------> 0.040505924667776395\n",
      "Epoch 77 / 100 ------> 0.0421272572668005\n",
      "Epoch 78 / 100 ------> 0.04067441427869056\n",
      "Epoch 79 / 100 ------> 0.040933786152256756\n",
      "Epoch 80 / 100 ------> 0.043049881748963854\n",
      "Epoch 81 / 100 ------> 0.04681593870516098\n",
      "Epoch 82 / 100 ------> 0.0426990784041798\n",
      "Epoch 83 / 100 ------> 0.04169842945545861\n",
      "Epoch 84 / 100 ------> 0.04071122426056686\n",
      "Epoch 85 / 100 ------> 0.038640712130827704\n",
      "Epoch 86 / 100 ------> 0.04237722059613992\n",
      "Epoch 87 / 100 ------> 0.041440384277380095\n",
      "Epoch 88 / 100 ------> 0.03973922578752443\n",
      "Epoch 89 / 100 ------> 0.040298091795919026\n",
      "Epoch 90 / 100 ------> 0.03934033116388409\n",
      "Epoch 91 / 100 ------> 0.04156452998717758\n",
      "Epoch 92 / 100 ------> 0.03888506988444604\n",
      "Epoch 93 / 100 ------> 0.04225134659261559\n",
      "Epoch 94 / 100 ------> 0.04855692410483368\n",
      "Epoch 95 / 100 ------> 0.04113481061409487\n",
      "Epoch 96 / 100 ------> 0.03744278270550953\n",
      "Epoch 97 / 100 ------> 0.042726406678391934\n",
      "Epoch 98 / 100 ------> 0.03972460581746319\n",
      "Epoch 99 / 100 ------> 0.03958835484188532\n",
      "Epoch 100 / 100 ------> 0.0392210433331079\n",
      "sgd_loss_record [0.2020103054530172, 0.08895907286582184, 0.07594480001833694, 0.073295522156909, 0.06370352294970562, 0.05919164928574551, 0.05913900437219594, 0.05826423717421941, 0.05858020484321446, 0.06087374696051452, 0.06293771620803511, 0.05445698157917128, 0.053315121661762645, 0.057631994248346335, 0.050641841539062996, 0.051386533592840206, 0.05216469953002802, 0.061976315120602074, 0.0597619170532856, 0.051821025717605225, 0.04936917098741923, 0.052384492101023826, 0.0486159093712775, 0.047557879921313995, 0.05013443276455654, 0.047832166433937756, 0.04678844014674751, 0.05526679518458867, 0.04665934199293531, 0.05875595101750927, 0.053739202473935294, 0.04700221674523138, 0.04789771247481931, 0.046707574937090386, 0.04949347124379247, 0.047638623952877386, 0.04511177974094944, 0.044762422681394, 0.04940741826842766, 0.05088644880491742, 0.04864876308877365, 0.045846769151004, 0.04500879838177982, 0.04570218547748727, 0.053666773514007046, 0.04587913369578108, 0.046720351713432716, 0.04828424653373514, 0.04461506718341773, 0.0456867922150884, 0.04610042790411315, 0.043305703291858, 0.042520795078451495, 0.0423521024494201, 0.04466620595143589, 0.04307292125050517, 0.04578055050088459, 0.046097592343684206, 0.04911728882218681, 0.04068193447960834, 0.042739378698825346, 0.042846036500435315, 0.04170172784807263, 0.045404399444226556, 0.04386653171948104, 0.04479008576417744, 0.040392376778858076, 0.04208044593958689, 0.042176149420118755, 0.04094329298505463, 0.04012747807895058, 0.05099796924227082, 0.042674727813698785, 0.04171793033443865, 0.041850257652460414, 0.040505924667776395, 0.0421272572668005, 0.04067441427869056, 0.040933786152256756, 0.043049881748963854, 0.04681593870516098, 0.0426990784041798, 0.04169842945545861, 0.04071122426056686, 0.038640712130827704, 0.04237722059613992, 0.041440384277380095, 0.03973922578752443, 0.040298091795919026, 0.03934033116388409, 0.04156452998717758, 0.03888506988444604, 0.04225134659261559, 0.04855692410483368, 0.04113481061409487, 0.03744278270550953, 0.042726406678391934, 0.03972460581746319, 0.03958835484188532, 0.0392210433331079]\n",
      "length of sgd_loss_record -> 100\n",
      "[[-0.29597809]\n",
      " [-0.2500507 ]\n",
      " [-0.58441885]\n",
      " [ 0.98098309]\n",
      " [ 0.43742287]\n",
      " [-4.25668416]\n",
      " [ 2.56497496]\n",
      " [ 3.73146351]\n",
      " [-1.31879292]\n",
      " [ 1.33968795]\n",
      " [ 4.83452722]\n",
      " [-0.76718161]\n",
      " [ 0.14434533]\n",
      " [ 1.89663286]\n",
      " [ 1.22159453]\n",
      " [ 0.61294792]\n",
      " [-2.48428573]\n",
      " [ 1.83883989]\n",
      " [-1.71594388]\n",
      " [-2.60429422]\n",
      " [ 3.5360087 ]\n",
      " [ 3.32858138]\n",
      " [-0.37787085]\n",
      " [ 0.89335744]\n",
      " [-0.17551493]\n",
      " [ 0.41521734]\n",
      " [ 2.99950336]\n",
      " [ 1.31313125]\n",
      " [ 3.81321639]\n",
      " [-0.17143359]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "sgd_weights,sgd_loss_record = sgd(x_train_array,y_train_array,0,0.1,w,100)\n",
    "print(sgd_weights,sgd_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 2.1174582673141273\n",
      "Epoch 2 / 100 ------> 0.651452825244891\n",
      "Epoch 3 / 100 ------> 0.6507229485850108\n",
      "Epoch 4 / 100 ------> 0.6508542158734975\n",
      "Epoch 5 / 100 ------> 0.6516656759524048\n",
      "Epoch 6 / 100 ------> 0.6515702683583343\n",
      "Epoch 7 / 100 ------> 0.6531447224481479\n",
      "Epoch 8 / 100 ------> 0.6515888104920866\n",
      "Epoch 9 / 100 ------> 0.6521605135131062\n",
      "Epoch 10 / 100 ------> 0.6507414039972977\n",
      "Epoch 11 / 100 ------> 0.651773176228631\n",
      "Epoch 12 / 100 ------> 0.6505534843631461\n",
      "Epoch 13 / 100 ------> 0.6511694196438556\n",
      "Epoch 14 / 100 ------> 0.6516675450707573\n",
      "Epoch 15 / 100 ------> 0.6509198516853361\n",
      "Epoch 16 / 100 ------> 0.6505152739123237\n",
      "Epoch 17 / 100 ------> 0.6520661238697293\n",
      "Epoch 18 / 100 ------> 0.6499299278412708\n",
      "Epoch 19 / 100 ------> 0.6515206780196237\n",
      "Epoch 20 / 100 ------> 0.6524881130418729\n",
      "Epoch 21 / 100 ------> 0.6504869958406043\n",
      "Epoch 22 / 100 ------> 0.6494789411560185\n",
      "Epoch 23 / 100 ------> 0.6491191184428288\n",
      "Epoch 24 / 100 ------> 0.6525495379160416\n",
      "Epoch 25 / 100 ------> 0.6497760945153795\n",
      "Epoch 26 / 100 ------> 0.6509775594771273\n",
      "Epoch 27 / 100 ------> 0.6511995736646838\n",
      "Epoch 28 / 100 ------> 0.651423073217588\n",
      "Epoch 29 / 100 ------> 0.6509958607854062\n",
      "Epoch 30 / 100 ------> 0.6519029667866978\n",
      "Epoch 31 / 100 ------> 0.6513035879630086\n",
      "Epoch 32 / 100 ------> 0.6508278871565584\n",
      "Epoch 33 / 100 ------> 0.6514110659032064\n",
      "Epoch 34 / 100 ------> 0.6502995258573145\n",
      "Epoch 35 / 100 ------> 0.6513839669979499\n",
      "Epoch 36 / 100 ------> 0.6506987778008632\n",
      "Epoch 37 / 100 ------> 0.6515974179961621\n",
      "Epoch 38 / 100 ------> 0.6516188001616439\n",
      "Epoch 39 / 100 ------> 0.6522799633080595\n",
      "Epoch 40 / 100 ------> 0.651848618174184\n",
      "Epoch 41 / 100 ------> 0.6511412957963466\n",
      "Epoch 42 / 100 ------> 0.6516825916998303\n",
      "Epoch 43 / 100 ------> 0.6504325419561335\n",
      "Epoch 44 / 100 ------> 0.6520239289908469\n",
      "Epoch 45 / 100 ------> 0.651962966854114\n",
      "Epoch 46 / 100 ------> 0.6508205832742255\n",
      "Epoch 47 / 100 ------> 0.6508836701334646\n",
      "Epoch 48 / 100 ------> 0.6516872272567997\n",
      "Epoch 49 / 100 ------> 0.651099324890435\n",
      "Epoch 50 / 100 ------> 0.6515289481178299\n",
      "Epoch 51 / 100 ------> 0.6508717158402739\n",
      "Epoch 52 / 100 ------> 0.6520528322711201\n",
      "Epoch 53 / 100 ------> 0.6509666769515466\n",
      "Epoch 54 / 100 ------> 0.6516513948404861\n",
      "Epoch 55 / 100 ------> 0.652526814532175\n",
      "Epoch 56 / 100 ------> 0.6515467668428578\n",
      "Epoch 57 / 100 ------> 0.6505583463315902\n",
      "Epoch 58 / 100 ------> 0.6504054632529553\n",
      "Epoch 59 / 100 ------> 0.6517936428446786\n",
      "Epoch 60 / 100 ------> 0.6512978245845802\n",
      "Epoch 61 / 100 ------> 0.6501627167684649\n",
      "Epoch 62 / 100 ------> 0.6507426253454279\n",
      "Epoch 63 / 100 ------> 0.6505840045922787\n",
      "Epoch 64 / 100 ------> 0.6523686295671435\n",
      "Epoch 65 / 100 ------> 0.6508898075599858\n",
      "Epoch 66 / 100 ------> 0.6514264008356419\n",
      "Epoch 67 / 100 ------> 0.6516395392650066\n",
      "Epoch 68 / 100 ------> 0.6502124705065262\n",
      "Epoch 69 / 100 ------> 0.6509186626460417\n",
      "Epoch 70 / 100 ------> 0.6516972193544922\n",
      "Epoch 71 / 100 ------> 0.6508557536691373\n",
      "Epoch 72 / 100 ------> 0.6523455532430215\n",
      "Epoch 73 / 100 ------> 0.6525807551587489\n",
      "Epoch 74 / 100 ------> 0.651593598060093\n",
      "Epoch 75 / 100 ------> 0.6501706568856697\n",
      "Epoch 76 / 100 ------> 0.6502590752472709\n",
      "Epoch 77 / 100 ------> 0.6510679732218069\n",
      "Epoch 78 / 100 ------> 0.6514664064733366\n",
      "Epoch 79 / 100 ------> 0.6502328722422779\n",
      "Epoch 80 / 100 ------> 0.6493017261852142\n",
      "Epoch 81 / 100 ------> 0.6511984828711112\n",
      "Epoch 82 / 100 ------> 0.6504428644592707\n",
      "Epoch 83 / 100 ------> 0.6519642021248963\n",
      "Epoch 84 / 100 ------> 0.6516128342545011\n",
      "Epoch 85 / 100 ------> 0.6509270923791902\n",
      "Epoch 86 / 100 ------> 0.6523208932625595\n",
      "Epoch 87 / 100 ------> 0.6511499454250316\n",
      "Epoch 88 / 100 ------> 0.651436134926307\n",
      "Epoch 89 / 100 ------> 0.6513463206193332\n",
      "Epoch 90 / 100 ------> 0.6508244138631246\n",
      "Epoch 91 / 100 ------> 0.6505468705738324\n",
      "Epoch 92 / 100 ------> 0.6507912326839905\n",
      "Epoch 93 / 100 ------> 0.6524046574102368\n",
      "Epoch 94 / 100 ------> 0.6517937701188379\n",
      "Epoch 95 / 100 ------> 0.6511520204378022\n",
      "Epoch 96 / 100 ------> 0.6520212840865461\n",
      "Epoch 97 / 100 ------> 0.6534337088885601\n",
      "Epoch 98 / 100 ------> 0.6495670598720897\n",
      "Epoch 99 / 100 ------> 0.6513131069932283\n",
      "Epoch 100 / 100 ------> 0.6515452956940535\n",
      "sgd_loss_record [2.1174582673141273, 0.651452825244891, 0.6507229485850108, 0.6508542158734975, 0.6516656759524048, 0.6515702683583343, 0.6531447224481479, 0.6515888104920866, 0.6521605135131062, 0.6507414039972977, 0.651773176228631, 0.6505534843631461, 0.6511694196438556, 0.6516675450707573, 0.6509198516853361, 0.6505152739123237, 0.6520661238697293, 0.6499299278412708, 0.6515206780196237, 0.6524881130418729, 0.6504869958406043, 0.6494789411560185, 0.6491191184428288, 0.6525495379160416, 0.6497760945153795, 0.6509775594771273, 0.6511995736646838, 0.651423073217588, 0.6509958607854062, 0.6519029667866978, 0.6513035879630086, 0.6508278871565584, 0.6514110659032064, 0.6502995258573145, 0.6513839669979499, 0.6506987778008632, 0.6515974179961621, 0.6516188001616439, 0.6522799633080595, 0.651848618174184, 0.6511412957963466, 0.6516825916998303, 0.6504325419561335, 0.6520239289908469, 0.651962966854114, 0.6508205832742255, 0.6508836701334646, 0.6516872272567997, 0.651099324890435, 0.6515289481178299, 0.6508717158402739, 0.6520528322711201, 0.6509666769515466, 0.6516513948404861, 0.652526814532175, 0.6515467668428578, 0.6505583463315902, 0.6504054632529553, 0.6517936428446786, 0.6512978245845802, 0.6501627167684649, 0.6507426253454279, 0.6505840045922787, 0.6523686295671435, 0.6508898075599858, 0.6514264008356419, 0.6516395392650066, 0.6502124705065262, 0.6509186626460417, 0.6516972193544922, 0.6508557536691373, 0.6523455532430215, 0.6525807551587489, 0.651593598060093, 0.6501706568856697, 0.6502590752472709, 0.6510679732218069, 0.6514664064733366, 0.6502328722422779, 0.6493017261852142, 0.6511984828711112, 0.6504428644592707, 0.6519642021248963, 0.6516128342545011, 0.6509270923791902, 0.6523208932625595, 0.6511499454250316, 0.651436134926307, 0.6513463206193332, 0.6508244138631246, 0.6505468705738324, 0.6507912326839905, 0.6524046574102368, 0.6517937701188379, 0.6511520204378022, 0.6520212840865461, 0.6534337088885601, 0.6495670598720897, 0.6513131069932283, 0.6515452956940535]\n",
      "length of sgd_loss_record -> 100\n",
      "[[ 0.02363283]\n",
      " [ 0.00993196]\n",
      " [ 0.0235761 ]\n",
      " [ 0.02249473]\n",
      " [ 0.01043315]\n",
      " [ 0.00856213]\n",
      " [ 0.02134111]\n",
      " [ 0.02180029]\n",
      " [ 0.01324097]\n",
      " [-0.01883408]\n",
      " [ 0.01958314]\n",
      " [ 0.00653322]\n",
      " [ 0.01900375]\n",
      " [ 0.01588582]\n",
      " [ 0.00350909]\n",
      " [ 0.00052119]\n",
      " [ 0.00849592]\n",
      " [ 0.00918267]\n",
      " [ 0.00196604]\n",
      " [-0.01155085]\n",
      " [ 0.02361163]\n",
      " [ 0.0121966 ]\n",
      " [ 0.02358079]\n",
      " [ 0.02104677]\n",
      " [ 0.00888694]\n",
      " [ 0.00596825]\n",
      " [ 0.01658028]\n",
      " [ 0.01789328]\n",
      " [ 0.01343937]\n",
      " [-0.01502634]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "sgd_weights_reg,sgd_loss_record_reg = sgd(x_train_array,y_train_array,15,0.01,w,100)\n",
    "print(sgd_weights_reg,sgd_weights_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Mini-Batch Gradient Descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: b-by-d matrix\n",
    "#     yi: label: b-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def mb_objective_gradient(w, xi, yi, lam):\n",
    "    n, d = xi.shape\n",
    "    yi = yi.reshape(n,1)\n",
    "    y_x = numpy.multiply(yi, xi) # n-by-d matrix\n",
    "    y_x_w = numpy.dot(y_x, w) # n-by-1 matrix\n",
    "    sum1 = numpy.exp(-y_x_w) # n-by-1 matrix\n",
    "    sum2 = numpy.log(1 + sum1) # n-by-1 matrix\n",
    "    loss = numpy.mean(sum2) # scalar\n",
    "    regularization_param = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    loss = loss+ regularization_param\n",
    "    \n",
    "    \n",
    "    \n",
    "    sum1 = numpy.exp(y_x_w) # n-by-1 matrix\n",
    "    sum2 = numpy.divide(y_x, 1+sum1) # n-by-d matrix\n",
    "    sum3 = -numpy.mean(sum2, axis=0).reshape(d, 1) # d-by-1 matrix\n",
    "    grad= sum3 + lam * w\n",
    "    \n",
    "    return loss,grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    a,b = x.shape\n",
    "    numpy.random.seed(42)\n",
    "    w = numpy.random.randn(b,1)\n",
    "    minibatch_size =50\n",
    "    mbgd_loss_record = []\n",
    "    for i in range(max_epoch):\n",
    "        for j in range(math.ceil(a/minibatch_size)):\n",
    "            shuffled_indices = numpy.random.permutation(minibatch_size)\n",
    "            x_shuffled = x[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            loss,gradients = mb_objective_gradient(w, x_shuffled, \n",
    "                                                           y_shuffled, lam)\n",
    "            \n",
    "            #print(\"gradients.shape->\",gradients.shape)\n",
    "            w = w - learning_rate*gradients\n",
    "            #print(\"w.shape->\",w.shape)\n",
    "        mbgd_loss_record.append(loss)\n",
    "        print(\"Epoch\", i+1,\"/\",max_epoch,\"------>\",loss)\n",
    "    print(\"mbgd_loss_record\",mbgd_loss_record)\n",
    "    print(\"length of mbgd_loss_record ->\",len(mbgd_loss_record))\n",
    "    return w,mbgd_loss_record\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 0.4371965052956125\n",
      "Epoch 2 / 100 ------> 0.25446104271781167\n",
      "Epoch 3 / 100 ------> 0.18591036728679747\n",
      "Epoch 4 / 100 ------> 0.1497454339712611\n",
      "Epoch 5 / 100 ------> 0.12768801312966852\n",
      "Epoch 6 / 100 ------> 0.11305590107666658\n",
      "Epoch 7 / 100 ------> 0.10266389115019908\n",
      "Epoch 8 / 100 ------> 0.09481304664830492\n",
      "Epoch 9 / 100 ------> 0.08855390110083226\n",
      "Epoch 10 / 100 ------> 0.08334107449972171\n",
      "Epoch 11 / 100 ------> 0.07885274694691162\n",
      "Epoch 12 / 100 ------> 0.07489311301769887\n",
      "Epoch 13 / 100 ------> 0.07133889188762761\n",
      "Epoch 14 / 100 ------> 0.06810950917201897\n",
      "Epoch 15 / 100 ------> 0.06515013476545531\n",
      "Epoch 16 / 100 ------> 0.06242182312538138\n",
      "Epoch 17 / 100 ------> 0.059895663076486194\n",
      "Epoch 18 / 100 ------> 0.05754924343998422\n",
      "Epoch 19 / 100 ------> 0.05536448521159504\n",
      "Epoch 20 / 100 ------> 0.05332629489503787\n",
      "Epoch 21 / 100 ------> 0.05142171813629938\n",
      "Epoch 22 / 100 ------> 0.04963940099174227\n",
      "Epoch 23 / 100 ------> 0.04796924129029054\n",
      "Epoch 24 / 100 ------> 0.04640215763518016\n",
      "Epoch 25 / 100 ------> 0.044929931172615296\n",
      "Epoch 26 / 100 ------> 0.043545092357461464\n",
      "Epoch 27 / 100 ------> 0.04224083562735597\n",
      "Epoch 28 / 100 ------> 0.04101095157326498\n",
      "Epoch 29 / 100 ------> 0.039849770346890245\n",
      "Epoch 30 / 100 ------> 0.03875211260044982\n",
      "Epoch 31 / 100 ------> 0.037713245801481644\n",
      "Epoch 32 / 100 ------> 0.03672884468173245\n",
      "Epoch 33 / 100 ------> 0.03579495510651424\n",
      "Epoch 34 / 100 ------> 0.03490796094293687\n",
      "Epoch 35 / 100 ------> 0.0340645536589443\n",
      "Epoch 36 / 100 ------> 0.033261704459999424\n",
      "Epoch 37 / 100 ------> 0.03249663880284139\n",
      "Epoch 38 / 100 ------> 0.03176681313729924\n",
      "Epoch 39 / 100 ------> 0.031069893729722697\n",
      "Epoch 40 / 100 ------> 0.030403737421602496\n",
      "Epoch 41 / 100 ------> 0.0297663741774945\n",
      "Epoch 42 / 100 ------> 0.029155991278669226\n",
      "Epoch 43 / 100 ------> 0.028570919023215412\n",
      "Epoch 44 / 100 ------> 0.028009617799386634\n",
      "Epoch 45 / 100 ------> 0.02747066640637011\n",
      "Epoch 46 / 100 ------> 0.026952751504907564\n",
      "Epoch 47 / 100 ------> 0.026454658088903748\n",
      "Epoch 48 / 100 ------> 0.025975260877969687\n",
      "Epoch 49 / 100 ------> 0.02551351653951463\n",
      "Epoch 50 / 100 ------> 0.025068456657331418\n",
      "Epoch 51 / 100 ------> 0.02463918137149442\n",
      "Epoch 52 / 100 ------> 0.024224853621736483\n",
      "Epoch 53 / 100 ------> 0.02382469393325092\n",
      "Epoch 54 / 100 ------> 0.023437975690073817\n",
      "Epoch 55 / 100 ------> 0.02306402084684913\n",
      "Epoch 56 / 100 ------> 0.022702196034890404\n",
      "Epoch 57 / 100 ------> 0.02235190902305936\n",
      "Epoch 58 / 100 ------> 0.02201260549812175\n",
      "Epoch 59 / 100 ------> 0.021683766132950276\n",
      "Epoch 60 / 100 ------> 0.021364903914264356\n",
      "Epoch 61 / 100 ------> 0.021055561704562686\n",
      "Epoch 62 / 100 ------> 0.020755310015551368\n",
      "Epoch 63 / 100 ------> 0.020463744972732633\n",
      "Epoch 64 / 100 ------> 0.02018048645292481\n",
      "Epoch 65 / 100 ------> 0.01990517637836233\n",
      "Epoch 66 / 100 ------> 0.019637477152698278\n",
      "Epoch 67 / 100 ------> 0.019377070225725906\n",
      "Epoch 68 / 100 ------> 0.01912365477496783\n",
      "Epoch 69 / 100 ------> 0.018876946493469885\n",
      "Epoch 70 / 100 ------> 0.018636676474199563\n",
      "Epoch 71 / 100 ------> 0.01840259018239724\n",
      "Epoch 72 / 100 ------> 0.01817444650807692\n",
      "Epoch 73 / 100 ------> 0.017952016891632446\n",
      "Epoch 74 / 100 ------> 0.01773508451618482\n",
      "Epoch 75 / 100 ------> 0.017523443560915578\n",
      "Epoch 76 / 100 ------> 0.017316898510176728\n",
      "Epoch 77 / 100 ------> 0.01711526351365941\n",
      "Epoch 78 / 100 ------> 0.016918361793342333\n",
      "Epoch 79 / 100 ------> 0.01672602509333785\n",
      "Epoch 80 / 100 ------> 0.016538093169109716\n",
      "Epoch 81 / 100 ------> 0.016354413312857027\n",
      "Epoch 82 / 100 ------> 0.016174839912148062\n",
      "Epoch 83 / 100 ------> 0.01599923403914874\n",
      "Epoch 84 / 100 ------> 0.015827463068025426\n",
      "Epoch 85 / 100 ------> 0.0156594003183149\n",
      "Epoch 86 / 100 ------> 0.01549492472224627\n",
      "Epoch 87 / 100 ------> 0.015333920514173984\n",
      "Epoch 88 / 100 ------> 0.01517627694043851\n",
      "Epoch 89 / 100 ------> 0.015021887988114267\n",
      "Epoch 90 / 100 ------> 0.014870652131234015\n",
      "Epoch 91 / 100 ------> 0.014722472093196532\n",
      "Epoch 92 / 100 ------> 0.014577254624171768\n",
      "Epoch 93 / 100 ------> 0.014434910292414066\n",
      "Epoch 94 / 100 ------> 0.014295353288483463\n",
      "Epoch 95 / 100 ------> 0.01415850124145555\n",
      "Epoch 96 / 100 ------> 0.014024275046272915\n",
      "Epoch 97 / 100 ------> 0.013892598701459919\n",
      "Epoch 98 / 100 ------> 0.013763399156482059\n",
      "Epoch 99 / 100 ------> 0.013636606168087984\n",
      "Epoch 100 / 100 ------> 0.013512152165023843\n",
      "mbgd_loss_record [0.4371965052956125, 0.25446104271781167, 0.18591036728679747, 0.1497454339712611, 0.12768801312966852, 0.11305590107666658, 0.10266389115019908, 0.09481304664830492, 0.08855390110083226, 0.08334107449972171, 0.07885274694691162, 0.07489311301769887, 0.07133889188762761, 0.06810950917201897, 0.06515013476545531, 0.06242182312538138, 0.059895663076486194, 0.05754924343998422, 0.05536448521159504, 0.05332629489503787, 0.05142171813629938, 0.04963940099174227, 0.04796924129029054, 0.04640215763518016, 0.044929931172615296, 0.043545092357461464, 0.04224083562735597, 0.04101095157326498, 0.039849770346890245, 0.03875211260044982, 0.037713245801481644, 0.03672884468173245, 0.03579495510651424, 0.03490796094293687, 0.0340645536589443, 0.033261704459999424, 0.03249663880284139, 0.03176681313729924, 0.031069893729722697, 0.030403737421602496, 0.0297663741774945, 0.029155991278669226, 0.028570919023215412, 0.028009617799386634, 0.02747066640637011, 0.026952751504907564, 0.026454658088903748, 0.025975260877969687, 0.02551351653951463, 0.025068456657331418, 0.02463918137149442, 0.024224853621736483, 0.02382469393325092, 0.023437975690073817, 0.02306402084684913, 0.022702196034890404, 0.02235190902305936, 0.02201260549812175, 0.021683766132950276, 0.021364903914264356, 0.021055561704562686, 0.020755310015551368, 0.020463744972732633, 0.02018048645292481, 0.01990517637836233, 0.019637477152698278, 0.019377070225725906, 0.01912365477496783, 0.018876946493469885, 0.018636676474199563, 0.01840259018239724, 0.01817444650807692, 0.017952016891632446, 0.01773508451618482, 0.017523443560915578, 0.017316898510176728, 0.01711526351365941, 0.016918361793342333, 0.01672602509333785, 0.016538093169109716, 0.016354413312857027, 0.016174839912148062, 0.01599923403914874, 0.015827463068025426, 0.0156594003183149, 0.01549492472224627, 0.015333920514173984, 0.01517627694043851, 0.015021887988114267, 0.014870652131234015, 0.014722472093196532, 0.014577254624171768, 0.014434910292414066, 0.014295353288483463, 0.01415850124145555, 0.014024275046272915, 0.013892598701459919, 0.013763399156482059, 0.013636606168087984, 0.013512152165023843]\n",
      "length of mbgd_loss_record -> 100\n",
      "[[ 0.8860522 ]\n",
      " [ 1.24958636]\n",
      " [ 1.06575071]\n",
      " [ 2.14663842]\n",
      " [-0.45556921]\n",
      " [ 0.10051609]\n",
      " [ 2.32203129]\n",
      " [ 1.04889112]\n",
      " [-0.48419222]\n",
      " [ 1.39300473]\n",
      " [ 0.40233252]\n",
      " [ 0.34779672]\n",
      " [ 1.05466811]\n",
      " [-1.09227724]\n",
      " [-0.51543954]\n",
      " [-0.54303598]\n",
      " [-0.76189868]\n",
      " [ 0.20984451]\n",
      " [ 0.03679176]\n",
      " [-0.92159949]\n",
      " [ 2.16868639]\n",
      " [ 1.08000964]\n",
      " [ 0.76771174]\n",
      " [-0.52589589]\n",
      " [ 0.13842923]\n",
      " [ 0.50288318]\n",
      " [-0.58352053]\n",
      " [ 0.53881614]\n",
      " [ 0.11539529]\n",
      " [ 0.6459991 ]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "mbgd_weights,mbgd_loss_record = mbgd(x_train_array,y_train_array,0,0.1,w,100)\n",
    "print(mbgd_weights,mbgd_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 10.339896406427007\n",
      "Epoch 2 / 100 ------> 0.9746487431324338\n",
      "Epoch 3 / 100 ------> 0.6309744988862965\n",
      "Epoch 4 / 100 ------> 0.6183631746767992\n",
      "Epoch 5 / 100 ------> 0.6178955323121096\n",
      "Epoch 6 / 100 ------> 0.6178780474499295\n",
      "Epoch 7 / 100 ------> 0.6178773896247677\n",
      "Epoch 8 / 100 ------> 0.6178773647601973\n",
      "Epoch 9 / 100 ------> 0.6178773638170671\n",
      "Epoch 10 / 100 ------> 0.6178773637811984\n",
      "Epoch 11 / 100 ------> 0.6178773637798316\n",
      "Epoch 12 / 100 ------> 0.6178773637797794\n",
      "Epoch 13 / 100 ------> 0.6178773637797773\n",
      "Epoch 14 / 100 ------> 0.6178773637797773\n",
      "Epoch 15 / 100 ------> 0.6178773637797774\n",
      "Epoch 16 / 100 ------> 0.6178773637797773\n",
      "Epoch 17 / 100 ------> 0.6178773637797773\n",
      "Epoch 18 / 100 ------> 0.6178773637797773\n",
      "Epoch 19 / 100 ------> 0.6178773637797774\n",
      "Epoch 20 / 100 ------> 0.6178773637797773\n",
      "Epoch 21 / 100 ------> 0.6178773637797774\n",
      "Epoch 22 / 100 ------> 0.6178773637797773\n",
      "Epoch 23 / 100 ------> 0.6178773637797772\n",
      "Epoch 24 / 100 ------> 0.6178773637797774\n",
      "Epoch 25 / 100 ------> 0.6178773637797774\n",
      "Epoch 26 / 100 ------> 0.6178773637797772\n",
      "Epoch 27 / 100 ------> 0.6178773637797772\n",
      "Epoch 28 / 100 ------> 0.6178773637797773\n",
      "Epoch 29 / 100 ------> 0.6178773637797773\n",
      "Epoch 30 / 100 ------> 0.6178773637797772\n",
      "Epoch 31 / 100 ------> 0.6178773637797773\n",
      "Epoch 32 / 100 ------> 0.6178773637797774\n",
      "Epoch 33 / 100 ------> 0.6178773637797774\n",
      "Epoch 34 / 100 ------> 0.6178773637797773\n",
      "Epoch 35 / 100 ------> 0.6178773637797772\n",
      "Epoch 36 / 100 ------> 0.6178773637797773\n",
      "Epoch 37 / 100 ------> 0.6178773637797773\n",
      "Epoch 38 / 100 ------> 0.6178773637797773\n",
      "Epoch 39 / 100 ------> 0.6178773637797772\n",
      "Epoch 40 / 100 ------> 0.6178773637797773\n",
      "Epoch 41 / 100 ------> 0.6178773637797773\n",
      "Epoch 42 / 100 ------> 0.6178773637797772\n",
      "Epoch 43 / 100 ------> 0.6178773637797773\n",
      "Epoch 44 / 100 ------> 0.6178773637797772\n",
      "Epoch 45 / 100 ------> 0.6178773637797773\n",
      "Epoch 46 / 100 ------> 0.6178773637797773\n",
      "Epoch 47 / 100 ------> 0.6178773637797773\n",
      "Epoch 48 / 100 ------> 0.6178773637797774\n",
      "Epoch 49 / 100 ------> 0.6178773637797773\n",
      "Epoch 50 / 100 ------> 0.6178773637797772\n",
      "Epoch 51 / 100 ------> 0.6178773637797772\n",
      "Epoch 52 / 100 ------> 0.6178773637797772\n",
      "Epoch 53 / 100 ------> 0.6178773637797773\n",
      "Epoch 54 / 100 ------> 0.6178773637797772\n",
      "Epoch 55 / 100 ------> 0.6178773637797772\n",
      "Epoch 56 / 100 ------> 0.6178773637797773\n",
      "Epoch 57 / 100 ------> 0.6178773637797772\n",
      "Epoch 58 / 100 ------> 0.6178773637797773\n",
      "Epoch 59 / 100 ------> 0.6178773637797773\n",
      "Epoch 60 / 100 ------> 0.6178773637797773\n",
      "Epoch 61 / 100 ------> 0.6178773637797773\n",
      "Epoch 62 / 100 ------> 0.6178773637797773\n",
      "Epoch 63 / 100 ------> 0.6178773637797773\n",
      "Epoch 64 / 100 ------> 0.6178773637797773\n",
      "Epoch 65 / 100 ------> 0.6178773637797774\n",
      "Epoch 66 / 100 ------> 0.6178773637797773\n",
      "Epoch 67 / 100 ------> 0.6178773637797772\n",
      "Epoch 68 / 100 ------> 0.6178773637797772\n",
      "Epoch 69 / 100 ------> 0.6178773637797772\n",
      "Epoch 70 / 100 ------> 0.6178773637797773\n",
      "Epoch 71 / 100 ------> 0.6178773637797772\n",
      "Epoch 72 / 100 ------> 0.6178773637797773\n",
      "Epoch 73 / 100 ------> 0.6178773637797772\n",
      "Epoch 74 / 100 ------> 0.6178773637797774\n",
      "Epoch 75 / 100 ------> 0.6178773637797773\n",
      "Epoch 76 / 100 ------> 0.6178773637797773\n",
      "Epoch 77 / 100 ------> 0.6178773637797772\n",
      "Epoch 78 / 100 ------> 0.6178773637797773\n",
      "Epoch 79 / 100 ------> 0.6178773637797772\n",
      "Epoch 80 / 100 ------> 0.6178773637797772\n",
      "Epoch 81 / 100 ------> 0.6178773637797773\n",
      "Epoch 82 / 100 ------> 0.6178773637797772\n",
      "Epoch 83 / 100 ------> 0.6178773637797774\n",
      "Epoch 84 / 100 ------> 0.6178773637797772\n",
      "Epoch 85 / 100 ------> 0.6178773637797772\n",
      "Epoch 86 / 100 ------> 0.6178773637797772\n",
      "Epoch 87 / 100 ------> 0.6178773637797772\n",
      "Epoch 88 / 100 ------> 0.6178773637797773\n",
      "Epoch 89 / 100 ------> 0.6178773637797773\n",
      "Epoch 90 / 100 ------> 0.6178773637797772\n",
      "Epoch 91 / 100 ------> 0.6178773637797772\n",
      "Epoch 92 / 100 ------> 0.6178773637797773\n",
      "Epoch 93 / 100 ------> 0.6178773637797773\n",
      "Epoch 94 / 100 ------> 0.6178773637797773\n",
      "Epoch 95 / 100 ------> 0.6178773637797772\n",
      "Epoch 96 / 100 ------> 0.6178773637797773\n",
      "Epoch 97 / 100 ------> 0.6178773637797772\n",
      "Epoch 98 / 100 ------> 0.6178773637797773\n",
      "Epoch 99 / 100 ------> 0.6178773637797773\n",
      "Epoch 100 / 100 ------> 0.6178773637797773\n",
      "mbgd_loss_record [10.339896406427007, 0.9746487431324338, 0.6309744988862965, 0.6183631746767992, 0.6178955323121096, 0.6178780474499295, 0.6178773896247677, 0.6178773647601973, 0.6178773638170671, 0.6178773637811984, 0.6178773637798316, 0.6178773637797794, 0.6178773637797773, 0.6178773637797773, 0.6178773637797774, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797774, 0.6178773637797773, 0.6178773637797774, 0.6178773637797773, 0.6178773637797772, 0.6178773637797774, 0.6178773637797774, 0.6178773637797772, 0.6178773637797772, 0.6178773637797773, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797774, 0.6178773637797774, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797774, 0.6178773637797773, 0.6178773637797772, 0.6178773637797772, 0.6178773637797772, 0.6178773637797773, 0.6178773637797772, 0.6178773637797772, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797774, 0.6178773637797773, 0.6178773637797772, 0.6178773637797772, 0.6178773637797772, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797772, 0.6178773637797774, 0.6178773637797773, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797772, 0.6178773637797772, 0.6178773637797773, 0.6178773637797772, 0.6178773637797774, 0.6178773637797772, 0.6178773637797772, 0.6178773637797772, 0.6178773637797772, 0.6178773637797773, 0.6178773637797773, 0.6178773637797772, 0.6178773637797772, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797772, 0.6178773637797773, 0.6178773637797773, 0.6178773637797773]\n",
      "length of mbgd_loss_record -> 100\n",
      "[[ 0.02194012]\n",
      " [ 0.0138974 ]\n",
      " [ 0.02241112]\n",
      " [ 0.02022569]\n",
      " [ 0.01445264]\n",
      " [ 0.02011069]\n",
      " [ 0.01948894]\n",
      " [ 0.02252229]\n",
      " [ 0.00598474]\n",
      " [ 0.00228228]\n",
      " [ 0.01163299]\n",
      " [-0.0047242 ]\n",
      " [ 0.0134743 ]\n",
      " [ 0.01280739]\n",
      " [-0.00182269]\n",
      " [ 0.00515901]\n",
      " [ 0.00219992]\n",
      " [ 0.00934194]\n",
      " [-0.00828453]\n",
      " [-0.00130326]\n",
      " [ 0.02372942]\n",
      " [ 0.01569045]\n",
      " [ 0.02474395]\n",
      " [ 0.02216202]\n",
      " [ 0.01655383]\n",
      " [ 0.02173708]\n",
      " [ 0.02085575]\n",
      " [ 0.02523257]\n",
      " [ 0.01014385]\n",
      " [ 0.01397146]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "mbgd_weights_reg,mbgd_loss_record_reg = mbgd(x_train_array,y_train_array,15,0.01,w,100)\n",
    "print(mbgd_weights_reg,mbgd_weights_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD, SGD, MBGD\n",
    "\n",
    "### Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.figure(figsize=(7,4))\n",
    "epochs = range(100)\n",
    "plt.plot(epochs,gd_loss_record,'b',label='GD')\n",
    "plt.plot(epochs,sgd_loss_record,'r',label='SGD')\n",
    "plt.plot(epochs,mbgd_loss_record,'g',label='MBGD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.axis([0, 100, 0, 1.0])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot with Regularization\n",
    "\n",
    "epochs = range(100)\n",
    "plt.plot(epochs,gd_loss_record_reg,'b',label='GD')\n",
    "plt.plot(epochs,sgd_loss_record_reg,'r',label='SGD')\n",
    "plt.plot(epochs,mbgd_loss_record_reg,'g',label='MBGD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.axis([0, 100, 0, 1])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction\n",
    "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     X: data: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    predict =X.T*w\n",
    "    print(\"predict.shape->\", predict.shape)\n",
    "    predict =predict.sum(axis=0)\n",
    "    print(\"predict.shape->\", predict.shape)\n",
    "    predict[predict>0] = 1\n",
    "    predict[predict<0] = -1\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict.shape-> (30, 456)\n",
      "predict.shape-> (456,)\n",
      "\n",
      "Training Set Accuracy: 92.54385964912281\n",
      "\n",
      "Training Set Error Rate: 7.456140350877192\n",
      "\n",
      "Training Set Classification Errors: 35\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error of logistric regression and regularized version\n",
    "predictions = predict(gd_weights,x_train_array)\n",
    "accuracy = numpy.mean(predictions == y_train_array) * 100\n",
    "print('\\nTraining Set Accuracy:',accuracy);\n",
    "print('\\nTraining Set Error Rate:', 100- accuracy);\n",
    "print('\\nTraining Set Classification Errors:',math.ceil((1-accuracy/100)*len(y_train_array)) );\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict.shape-> (30, 456)\n",
      "predict.shape-> (456,)\n",
      "\n",
      "Training Set With Regularization Accuracy: 92.98245614035088\n",
      "\n",
      "Training Set With Regularization Error Rate: 7.017543859649123\n",
      "\n",
      "Training Set With Regularization Classification Errors: 32\n"
     ]
    }
   ],
   "source": [
    "predictions_reg = predict(gd_weights_reg,x_train_array)\n",
    "\n",
    "accuracy = numpy.mean(predictions_reg == y_train_array) * 100\n",
    "print('\\nTraining Set With Regularization Accuracy:',accuracy);\n",
    "print('\\nTraining Set With Regularization Error Rate:', 100- accuracy);\n",
    "print('\\nTraining Set With Regularization Classification Errors:',math.ceil((1-accuracy/100)*len(y_train_array)) );\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict.shape-> (30, 113)\n",
      "predict.shape-> (113,)\n",
      "\n",
      "Test Set Accuracy: 94.69026548672566\n",
      "\n",
      "Test Set Error Rate: 5.309734513274336\n",
      "\n",
      "Test Set Classification Errors: 25\n"
     ]
    }
   ],
   "source": [
    "# evaluate testing error of logistric regression and regularized version\n",
    "test_predictions = predict(gd_weights,x_test)\n",
    "accuracy = numpy.mean(test_predictions == y_test) * 100\n",
    "print('\\nTest Set Accuracy:',accuracy);\n",
    "print('\\nTest Set Error Rate:', 100- accuracy);\n",
    "print('\\nTest Set Classification Errors:',math.ceil((1-accuracy/100)*len(y_train_array)) );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict.shape-> (30, 113)\n",
      "predict.shape-> (113,)\n",
      "\n",
      "Test Set Accuracy with Regularization : 96.46017699115043\n",
      "\n",
      "Test Set Error Rate with Regularization: 3.5398230088495666\n",
      "\n",
      "Test Set Classification Errors with Regularization: 17\n"
     ]
    }
   ],
   "source": [
    "test_predictions_reg = predict(gd_weights_reg,x_test)\n",
    "accuracy = numpy.mean(test_predictions_reg == y_test) * 100\n",
    "print('\\nTest Set Accuracy with Regularization :',accuracy);\n",
    "print('\\nTest Set Error Rate with Regularization:', 100- accuracy);\n",
    "print('\\nTest Set Classification Errors with Regularization:',math.ceil((1-accuracy/100)*len(y_train_array)) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 124.45161615922218\n",
      "Epoch 2 / 100 ------> 100.83475385352784\n",
      "Epoch 3 / 100 ------> 81.7102697605031\n",
      "Epoch 4 / 100 ------> 66.22482724297049\n",
      "Epoch 5 / 100 ------> 53.68717103560218\n",
      "Epoch 6 / 100 ------> 43.537323903929604\n",
      "Epoch 7 / 100 ------> 35.32163751331024\n",
      "Epoch 8 / 100 ------> 28.672585518252927\n",
      "Epoch 9 / 100 ------> 23.292398460904526\n",
      "Epoch 10 / 100 ------> 18.93981133139054\n",
      "Epoch 11 / 100 ------> 15.419333066272177\n",
      "Epoch 12 / 100 ------> 12.572559013402076\n",
      "Epoch 13 / 100 ------> 10.271137606193143\n",
      "Epoch 14 / 100 ------> 8.411075500319537\n",
      "Epoch 15 / 100 ------> 6.908124882862803\n",
      "Epoch 16 / 100 ------> 5.694045459633097\n",
      "Epoch 17 / 100 ------> 4.713573766455173\n",
      "Epoch 18 / 100 ------> 3.9219651882652955\n",
      "Epoch 19 / 100 ------> 3.28300034041624\n",
      "Epoch 20 / 100 ------> 2.767368281767032\n",
      "Epoch 21 / 100 ------> 2.3513555372851864\n",
      "Epoch 22 / 100 ------> 2.0157831769053907\n",
      "Epoch 23 / 100 ------> 1.7451450140713938\n",
      "Epoch 24 / 100 ------> 1.5269088478836377\n",
      "Epoch 25 / 100 ------> 1.3509499032154872\n",
      "Epoch 26 / 100 ------> 1.2090914892009137\n",
      "Epoch 27 / 100 ------> 1.0947326412157097\n",
      "Epoch 28 / 100 ------> 1.0025463485337047\n",
      "Epoch 29 / 100 ------> 0.9282350739625688\n",
      "Epoch 30 / 100 ------> 0.8683327817532551\n",
      "Epoch 31 / 100 ------> 0.8200447182016444\n",
      "Epoch 32 / 100 ------> 0.7811178286832412\n",
      "Epoch 33 / 100 ------> 0.7497360224773929\n",
      "Epoch 34 / 100 ------> 0.7244355751028815\n",
      "Epoch 35 / 100 ------> 0.7040368363486271\n",
      "Epoch 36 / 100 ------> 0.6875891292154301\n",
      "Epoch 37 / 100 ------> 0.6743263106603111\n",
      "Epoch 38 / 100 ------> 0.6636309431944728\n",
      "Epoch 39 / 100 ------> 0.655005416266341\n",
      "Epoch 40 / 100 ------> 0.6480486737157785\n",
      "Epoch 41 / 100 ------> 0.6424374614302097\n",
      "Epoch 42 / 100 ------> 0.6379112184579406\n",
      "Epoch 43 / 100 ------> 0.6342599041772954\n",
      "Epoch 44 / 100 ------> 0.6313141910664676\n",
      "Epoch 45 / 100 ------> 0.6289375632447197\n",
      "Epoch 46 / 100 ------> 0.6270199502414909\n",
      "Epoch 47 / 100 ------> 0.6254725974637194\n",
      "Epoch 48 / 100 ------> 0.624223932884701\n",
      "Epoch 49 / 100 ------> 0.6232162362587748\n",
      "Epoch 50 / 100 ------> 0.6224029548535248\n",
      "Epoch 51 / 100 ------> 0.62174654004701\n",
      "Epoch 52 / 100 ------> 0.621216703585186\n",
      "Epoch 53 / 100 ------> 0.620789011982731\n",
      "Epoch 54 / 100 ------> 0.6204437534051813\n",
      "Epoch 55 / 100 ------> 0.6201650241380252\n",
      "Epoch 56 / 100 ------> 0.6199399920305495\n",
      "Epoch 57 / 100 ------> 0.6197583025830251\n",
      "Epoch 58 / 100 ------> 0.6196116000152265\n",
      "Epoch 59 / 100 ------> 0.6194931410262086\n",
      "Epoch 60 / 100 ------> 0.6193974832824802\n",
      "Epoch 61 / 100 ------> 0.6193202341576476\n",
      "Epoch 62 / 100 ------> 0.6192578480550568\n",
      "Epoch 63 / 100 ------> 0.6192074629078025\n",
      "Epoch 64 / 100 ------> 0.6191667682738436\n",
      "Epoch 65 / 100 ------> 0.6191338989133512\n",
      "Epoch 66 / 100 ------> 0.6191073489196529\n",
      "Epoch 67 / 100 ------> 0.6190859024296154\n",
      "Epoch 68 / 100 ------> 0.6190685777087059\n",
      "Epoch 69 / 100 ------> 0.6190545820262041\n",
      "Epoch 70 / 100 ------> 0.6190432752360707\n",
      "Epoch 71 / 100 ------> 0.6190341403821533\n",
      "Epoch 72 / 100 ------> 0.6190267599714943\n",
      "Epoch 73 / 100 ------> 0.6190207968216614\n",
      "Epoch 74 / 100 ------> 0.6190159785994419\n",
      "Epoch 75 / 100 ------> 0.6190120853387502\n",
      "Epoch 76 / 100 ------> 0.6190089393631425\n",
      "Epoch 77 / 100 ------> 0.6190063971492674\n",
      "Epoch 78 / 100 ------> 0.6190043427570824\n",
      "Epoch 79 / 100 ------> 0.6190026825248711\n",
      "Epoch 80 / 100 ------> 0.6190013407853504\n",
      "Epoch 81 / 100 ------> 0.6190002564061623\n",
      "Epoch 82 / 100 ------> 0.6189993799959714\n",
      "Epoch 83 / 100 ------> 0.6189986716480042\n",
      "Epoch 84 / 100 ------> 0.6189980991175612\n",
      "Epoch 85 / 100 ------> 0.6189976363499726\n",
      "Epoch 86 / 100 ------> 0.6189972622915598\n",
      "Epoch 87 / 100 ------> 0.6189969599291533\n",
      "Epoch 88 / 100 ------> 0.618996715514201\n",
      "Epoch 89 / 100 ------> 0.6189965179359649\n",
      "Epoch 90 / 100 ------> 0.6189963582151412\n",
      "Epoch 91 / 100 ------> 0.618996229094748\n",
      "Epoch 92 / 100 ------> 0.6189961247095883\n",
      "Epoch 93 / 100 ------> 0.6189960403191836\n",
      "Epoch 94 / 100 ------> 0.618995972091985\n",
      "Epoch 95 / 100 ------> 0.6189959169310076\n",
      "Epoch 96 / 100 ------> 0.6189958723329355\n",
      "Epoch 97 / 100 ------> 0.6189958362742634\n",
      "Epoch 98 / 100 ------> 0.6189958071192877\n",
      "Epoch 99 / 100 ------> 0.6189957835457514\n",
      "Epoch 100 / 100 ------> 0.6189957644847507\n",
      "gd_loss_record [124.45161615922218, 100.83475385352784, 81.7102697605031, 66.22482724297049, 53.68717103560218, 43.537323903929604, 35.32163751331024, 28.672585518252927, 23.292398460904526, 18.93981133139054, 15.419333066272177, 12.572559013402076, 10.271137606193143, 8.411075500319537, 6.908124882862803, 5.694045459633097, 4.713573766455173, 3.9219651882652955, 3.28300034041624, 2.767368281767032, 2.3513555372851864, 2.0157831769053907, 1.7451450140713938, 1.5269088478836377, 1.3509499032154872, 1.2090914892009137, 1.0947326412157097, 1.0025463485337047, 0.9282350739625688, 0.8683327817532551, 0.8200447182016444, 0.7811178286832412, 0.7497360224773929, 0.7244355751028815, 0.7040368363486271, 0.6875891292154301, 0.6743263106603111, 0.6636309431944728, 0.655005416266341, 0.6480486737157785, 0.6424374614302097, 0.6379112184579406, 0.6342599041772954, 0.6313141910664676, 0.6289375632447197, 0.6270199502414909, 0.6254725974637194, 0.624223932884701, 0.6232162362587748, 0.6224029548535248, 0.62174654004701, 0.621216703585186, 0.620789011982731, 0.6204437534051813, 0.6201650241380252, 0.6199399920305495, 0.6197583025830251, 0.6196116000152265, 0.6194931410262086, 0.6193974832824802, 0.6193202341576476, 0.6192578480550568, 0.6192074629078025, 0.6191667682738436, 0.6191338989133512, 0.6191073489196529, 0.6190859024296154, 0.6190685777087059, 0.6190545820262041, 0.6190432752360707, 0.6190341403821533, 0.6190267599714943, 0.6190207968216614, 0.6190159785994419, 0.6190120853387502, 0.6190089393631425, 0.6190063971492674, 0.6190043427570824, 0.6190026825248711, 0.6190013407853504, 0.6190002564061623, 0.6189993799959714, 0.6189986716480042, 0.6189980991175612, 0.6189976363499726, 0.6189972622915598, 0.6189969599291533, 0.618996715514201, 0.6189965179359649, 0.6189963582151412, 0.618996229094748, 0.6189961247095883, 0.6189960403191836, 0.618995972091985, 0.6189959169310076, 0.6189958723329355, 0.6189958362742634, 0.6189958071192877, 0.6189957835457514, 0.6189957644847507]\n",
      "length of gd_loss_record -> 100\n",
      "[[ 0.02713193]\n",
      " [ 0.01652294]\n",
      " [ 0.02755809]\n",
      " [ 0.02606562]\n",
      " [ 0.01361186]\n",
      " [ 0.02109757]\n",
      " [ 0.02492039]\n",
      " [ 0.02901356]\n",
      " [ 0.0124619 ]\n",
      " [-0.0018676 ]\n",
      " [ 0.01969689]\n",
      " [-0.00038253]\n",
      " [ 0.01908926]\n",
      " [ 0.01857533]\n",
      " [-0.00279153]\n",
      " [ 0.0077432 ]\n",
      " [ 0.00638876]\n",
      " [ 0.01307176]\n",
      " [-0.00065168]\n",
      " [-0.00019105]\n",
      " [ 0.02902641]\n",
      " [ 0.01872169]\n",
      " [ 0.02916688]\n",
      " [ 0.02688328]\n",
      " [ 0.01655802]\n",
      " [ 0.02172418]\n",
      " [ 0.02404061]\n",
      " [ 0.02972593]\n",
      " [ 0.01706956]\n",
      " [ 0.01130228]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "#GD parrameter Tunning\n",
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "gd_weights_reg,gd_loss_record_reg = gradient_descent(x_train_array,y_train_array,10,0.01,w,100)\n",
    "print(gd_weights_reg,gd_weights_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 2.0584968167438484\n",
      "Epoch 2 / 100 ------> 0.6294958999808085\n",
      "Epoch 3 / 100 ------> 0.628520384539605\n",
      "Epoch 4 / 100 ------> 0.6293784085557854\n",
      "Epoch 5 / 100 ------> 0.6298367300819923\n",
      "Epoch 6 / 100 ------> 0.6296811514415074\n",
      "Epoch 7 / 100 ------> 0.6313440958612698\n",
      "Epoch 8 / 100 ------> 0.6299151796750977\n",
      "Epoch 9 / 100 ------> 0.6303145405950232\n",
      "Epoch 10 / 100 ------> 0.6290827111256909\n",
      "Epoch 11 / 100 ------> 0.6295988707372036\n",
      "Epoch 12 / 100 ------> 0.62881135445401\n",
      "Epoch 13 / 100 ------> 0.6292201371943082\n",
      "Epoch 14 / 100 ------> 0.6298679042811525\n",
      "Epoch 15 / 100 ------> 0.6292306798109196\n",
      "Epoch 16 / 100 ------> 0.6288794198280294\n",
      "Epoch 17 / 100 ------> 0.6304807207617374\n",
      "Epoch 18 / 100 ------> 0.6281352689938791\n",
      "Epoch 19 / 100 ------> 0.6291569009516619\n",
      "Epoch 20 / 100 ------> 0.6307999704927495\n",
      "Epoch 21 / 100 ------> 0.6282775155463596\n",
      "Epoch 22 / 100 ------> 0.6279108311666506\n",
      "Epoch 23 / 100 ------> 0.6269446734169175\n",
      "Epoch 24 / 100 ------> 0.6309673728963113\n",
      "Epoch 25 / 100 ------> 0.6279579517326846\n",
      "Epoch 26 / 100 ------> 0.6291429063085137\n",
      "Epoch 27 / 100 ------> 0.6294849885244335\n",
      "Epoch 28 / 100 ------> 0.629864043717563\n",
      "Epoch 29 / 100 ------> 0.6292428856699872\n",
      "Epoch 30 / 100 ------> 0.6299405602336288\n",
      "Epoch 31 / 100 ------> 0.6294334539887023\n",
      "Epoch 32 / 100 ------> 0.6288132887453769\n",
      "Epoch 33 / 100 ------> 0.6296143451984291\n",
      "Epoch 34 / 100 ------> 0.6284295872171279\n",
      "Epoch 35 / 100 ------> 0.6296622283881264\n",
      "Epoch 36 / 100 ------> 0.6289463982943062\n",
      "Epoch 37 / 100 ------> 0.6297853165182609\n",
      "Epoch 38 / 100 ------> 0.6301992163371636\n",
      "Epoch 39 / 100 ------> 0.6304808012400697\n",
      "Epoch 40 / 100 ------> 0.6298840965039533\n",
      "Epoch 41 / 100 ------> 0.6293366760188099\n",
      "Epoch 42 / 100 ------> 0.6298685084574158\n",
      "Epoch 43 / 100 ------> 0.6281342984684832\n",
      "Epoch 44 / 100 ------> 0.629913311595407\n",
      "Epoch 45 / 100 ------> 0.6302559014336725\n",
      "Epoch 46 / 100 ------> 0.6286751862432973\n",
      "Epoch 47 / 100 ------> 0.6290705221202697\n",
      "Epoch 48 / 100 ------> 0.6303277714927508\n",
      "Epoch 49 / 100 ------> 0.6295730597694545\n",
      "Epoch 50 / 100 ------> 0.6298293801893571\n",
      "Epoch 51 / 100 ------> 0.6289857771724159\n",
      "Epoch 52 / 100 ------> 0.6305349485531561\n",
      "Epoch 53 / 100 ------> 0.6292040481901621\n",
      "Epoch 54 / 100 ------> 0.629632548670613\n",
      "Epoch 55 / 100 ------> 0.6306153484800526\n",
      "Epoch 56 / 100 ------> 0.6300607995726243\n",
      "Epoch 57 / 100 ------> 0.628345848968265\n",
      "Epoch 58 / 100 ------> 0.6280152744387162\n",
      "Epoch 59 / 100 ------> 0.6297265229944246\n",
      "Epoch 60 / 100 ------> 0.6294915580497773\n",
      "Epoch 61 / 100 ------> 0.6280620144807121\n",
      "Epoch 62 / 100 ------> 0.6286189661513716\n",
      "Epoch 63 / 100 ------> 0.6285636803391575\n",
      "Epoch 64 / 100 ------> 0.6302773443536299\n",
      "Epoch 65 / 100 ------> 0.6286898260886866\n",
      "Epoch 66 / 100 ------> 0.6296027775858979\n",
      "Epoch 67 / 100 ------> 0.6297828041384625\n",
      "Epoch 68 / 100 ------> 0.6282018309385935\n",
      "Epoch 69 / 100 ------> 0.6288740747179371\n",
      "Epoch 70 / 100 ------> 0.6298466229622559\n",
      "Epoch 71 / 100 ------> 0.6291617273062958\n",
      "Epoch 72 / 100 ------> 0.6301905295366447\n",
      "Epoch 73 / 100 ------> 0.6304660604373236\n",
      "Epoch 74 / 100 ------> 0.6296639803456778\n",
      "Epoch 75 / 100 ------> 0.6284747737811995\n",
      "Epoch 76 / 100 ------> 0.6284015063735197\n",
      "Epoch 77 / 100 ------> 0.6290901608613625\n",
      "Epoch 78 / 100 ------> 0.6297321008970492\n",
      "Epoch 79 / 100 ------> 0.6288696606844135\n",
      "Epoch 80 / 100 ------> 0.6277413518352257\n",
      "Epoch 81 / 100 ------> 0.6291711537217687\n",
      "Epoch 82 / 100 ------> 0.6282736767057469\n",
      "Epoch 83 / 100 ------> 0.6303125059683756\n",
      "Epoch 84 / 100 ------> 0.6297124336246519\n",
      "Epoch 85 / 100 ------> 0.6289403885638931\n",
      "Epoch 86 / 100 ------> 0.6306858371804653\n",
      "Epoch 87 / 100 ------> 0.6292364889291936\n",
      "Epoch 88 / 100 ------> 0.629508405762884\n",
      "Epoch 89 / 100 ------> 0.6294799522160343\n",
      "Epoch 90 / 100 ------> 0.6291059304605243\n",
      "Epoch 91 / 100 ------> 0.6285607034603528\n",
      "Epoch 92 / 100 ------> 0.6287243782669764\n",
      "Epoch 93 / 100 ------> 0.6303768366173599\n",
      "Epoch 94 / 100 ------> 0.6302727681685109\n",
      "Epoch 95 / 100 ------> 0.6293494250258928\n",
      "Epoch 96 / 100 ------> 0.630001576025029\n",
      "Epoch 97 / 100 ------> 0.631490534368425\n",
      "Epoch 98 / 100 ------> 0.6276500135046372\n",
      "Epoch 99 / 100 ------> 0.6296230558469986\n",
      "Epoch 100 / 100 ------> 0.6296867068195673\n",
      "sgd_loss_record [2.0584968167438484, 0.6294958999808085, 0.628520384539605, 0.6293784085557854, 0.6298367300819923, 0.6296811514415074, 0.6313440958612698, 0.6299151796750977, 0.6303145405950232, 0.6290827111256909, 0.6295988707372036, 0.62881135445401, 0.6292201371943082, 0.6298679042811525, 0.6292306798109196, 0.6288794198280294, 0.6304807207617374, 0.6281352689938791, 0.6291569009516619, 0.6307999704927495, 0.6282775155463596, 0.6279108311666506, 0.6269446734169175, 0.6309673728963113, 0.6279579517326846, 0.6291429063085137, 0.6294849885244335, 0.629864043717563, 0.6292428856699872, 0.6299405602336288, 0.6294334539887023, 0.6288132887453769, 0.6296143451984291, 0.6284295872171279, 0.6296622283881264, 0.6289463982943062, 0.6297853165182609, 0.6301992163371636, 0.6304808012400697, 0.6298840965039533, 0.6293366760188099, 0.6298685084574158, 0.6281342984684832, 0.629913311595407, 0.6302559014336725, 0.6286751862432973, 0.6290705221202697, 0.6303277714927508, 0.6295730597694545, 0.6298293801893571, 0.6289857771724159, 0.6305349485531561, 0.6292040481901621, 0.629632548670613, 0.6306153484800526, 0.6300607995726243, 0.628345848968265, 0.6280152744387162, 0.6297265229944246, 0.6294915580497773, 0.6280620144807121, 0.6286189661513716, 0.6285636803391575, 0.6302773443536299, 0.6286898260886866, 0.6296027775858979, 0.6297828041384625, 0.6282018309385935, 0.6288740747179371, 0.6298466229622559, 0.6291617273062958, 0.6301905295366447, 0.6304660604373236, 0.6296639803456778, 0.6284747737811995, 0.6284015063735197, 0.6290901608613625, 0.6297321008970492, 0.6288696606844135, 0.6277413518352257, 0.6291711537217687, 0.6282736767057469, 0.6303125059683756, 0.6297124336246519, 0.6289403885638931, 0.6306858371804653, 0.6292364889291936, 0.629508405762884, 0.6294799522160343, 0.6291059304605243, 0.6285607034603528, 0.6287243782669764, 0.6303768366173599, 0.6302727681685109, 0.6293494250258928, 0.630001576025029, 0.631490534368425, 0.6276500135046372, 0.6296230558469986, 0.6296867068195673]\n",
      "length of sgd_loss_record -> 100\n",
      "[[ 0.03259466]\n",
      " [ 0.0131003 ]\n",
      " [ 0.03255496]\n",
      " [ 0.03131746]\n",
      " [ 0.01527949]\n",
      " [ 0.01323305]\n",
      " [ 0.02928476]\n",
      " [ 0.02979513]\n",
      " [ 0.01551523]\n",
      " [-0.02150641]\n",
      " [ 0.0267322 ]\n",
      " [ 0.00883291]\n",
      " [ 0.02667393]\n",
      " [ 0.02211607]\n",
      " [ 0.00900853]\n",
      " [ 0.00195293]\n",
      " [ 0.01286186]\n",
      " [ 0.01487533]\n",
      " [ 0.00259412]\n",
      " [-0.01181205]\n",
      " [ 0.03250501]\n",
      " [ 0.01642669]\n",
      " [ 0.03280078]\n",
      " [ 0.02921578]\n",
      " [ 0.01393404]\n",
      " [ 0.00964879]\n",
      " [ 0.02336566]\n",
      " [ 0.02569495]\n",
      " [ 0.0171955 ]\n",
      " [-0.01520785]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "#SGD Parameter Tunning\n",
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "sgd_weights_reg,sgd_loss_record_reg = sgd(x_train_array,y_train_array,10,0.01,w,100)\n",
    "print(sgd_weights_reg,sgd_weights_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 ------> 18.87208318824271\n",
      "Epoch 2 / 100 ------> 2.73180985220011\n",
      "Epoch 3 / 100 ------> 0.838492394853013\n",
      "Epoch 4 / 100 ------> 0.6198808968356218\n",
      "Epoch 5 / 100 ------> 0.5945208719315535\n",
      "Epoch 6 / 100 ------> 0.5915548375381656\n",
      "Epoch 7 / 100 ------> 0.5912056079118679\n",
      "Epoch 8 / 100 ------> 0.5911642757153739\n",
      "Epoch 9 / 100 ------> 0.5911593645668601\n",
      "Epoch 10 / 100 ------> 0.5911587792447626\n",
      "Epoch 11 / 100 ------> 0.591158709321209\n",
      "Epoch 12 / 100 ------> 0.5911587009527722\n",
      "Epoch 13 / 100 ------> 0.591158699949797\n",
      "Epoch 14 / 100 ------> 0.5911586998294496\n",
      "Epoch 15 / 100 ------> 0.5911586998149955\n",
      "Epoch 16 / 100 ------> 0.5911586998132581\n",
      "Epoch 17 / 100 ------> 0.5911586998130491\n",
      "Epoch 18 / 100 ------> 0.591158699813024\n",
      "Epoch 19 / 100 ------> 0.5911586998130209\n",
      "Epoch 20 / 100 ------> 0.5911586998130206\n",
      "Epoch 21 / 100 ------> 0.5911586998130206\n",
      "Epoch 22 / 100 ------> 0.5911586998130205\n",
      "Epoch 23 / 100 ------> 0.5911586998130206\n",
      "Epoch 24 / 100 ------> 0.5911586998130207\n",
      "Epoch 25 / 100 ------> 0.5911586998130206\n",
      "Epoch 26 / 100 ------> 0.5911586998130205\n",
      "Epoch 27 / 100 ------> 0.5911586998130207\n",
      "Epoch 28 / 100 ------> 0.5911586998130205\n",
      "Epoch 29 / 100 ------> 0.5911586998130205\n",
      "Epoch 30 / 100 ------> 0.5911586998130206\n",
      "Epoch 31 / 100 ------> 0.5911586998130205\n",
      "Epoch 32 / 100 ------> 0.5911586998130205\n",
      "Epoch 33 / 100 ------> 0.5911586998130205\n",
      "Epoch 34 / 100 ------> 0.5911586998130206\n",
      "Epoch 35 / 100 ------> 0.5911586998130206\n",
      "Epoch 36 / 100 ------> 0.5911586998130206\n",
      "Epoch 37 / 100 ------> 0.5911586998130206\n",
      "Epoch 38 / 100 ------> 0.5911586998130206\n",
      "Epoch 39 / 100 ------> 0.5911586998130206\n",
      "Epoch 40 / 100 ------> 0.5911586998130206\n",
      "Epoch 41 / 100 ------> 0.5911586998130206\n",
      "Epoch 42 / 100 ------> 0.5911586998130206\n",
      "Epoch 43 / 100 ------> 0.5911586998130206\n",
      "Epoch 44 / 100 ------> 0.5911586998130206\n",
      "Epoch 45 / 100 ------> 0.5911586998130206\n",
      "Epoch 46 / 100 ------> 0.5911586998130206\n",
      "Epoch 47 / 100 ------> 0.5911586998130206\n",
      "Epoch 48 / 100 ------> 0.5911586998130206\n",
      "Epoch 49 / 100 ------> 0.5911586998130206\n",
      "Epoch 50 / 100 ------> 0.5911586998130206\n",
      "Epoch 51 / 100 ------> 0.5911586998130206\n",
      "Epoch 52 / 100 ------> 0.5911586998130205\n",
      "Epoch 53 / 100 ------> 0.5911586998130206\n",
      "Epoch 54 / 100 ------> 0.5911586998130206\n",
      "Epoch 55 / 100 ------> 0.5911586998130206\n",
      "Epoch 56 / 100 ------> 0.5911586998130206\n",
      "Epoch 57 / 100 ------> 0.5911586998130205\n",
      "Epoch 58 / 100 ------> 0.5911586998130206\n",
      "Epoch 59 / 100 ------> 0.5911586998130206\n",
      "Epoch 60 / 100 ------> 0.5911586998130206\n",
      "Epoch 61 / 100 ------> 0.5911586998130206\n",
      "Epoch 62 / 100 ------> 0.5911586998130207\n",
      "Epoch 63 / 100 ------> 0.5911586998130206\n",
      "Epoch 64 / 100 ------> 0.5911586998130206\n",
      "Epoch 65 / 100 ------> 0.5911586998130206\n",
      "Epoch 66 / 100 ------> 0.5911586998130206\n",
      "Epoch 67 / 100 ------> 0.5911586998130206\n",
      "Epoch 68 / 100 ------> 0.5911586998130206\n",
      "Epoch 69 / 100 ------> 0.5911586998130206\n",
      "Epoch 70 / 100 ------> 0.5911586998130206\n",
      "Epoch 71 / 100 ------> 0.5911586998130206\n",
      "Epoch 72 / 100 ------> 0.5911586998130206\n",
      "Epoch 73 / 100 ------> 0.5911586998130206\n",
      "Epoch 74 / 100 ------> 0.5911586998130206\n",
      "Epoch 75 / 100 ------> 0.5911586998130206\n",
      "Epoch 76 / 100 ------> 0.5911586998130206\n",
      "Epoch 77 / 100 ------> 0.5911586998130206\n",
      "Epoch 78 / 100 ------> 0.5911586998130206\n",
      "Epoch 79 / 100 ------> 0.5911586998130206\n",
      "Epoch 80 / 100 ------> 0.5911586998130206\n",
      "Epoch 81 / 100 ------> 0.5911586998130206\n",
      "Epoch 82 / 100 ------> 0.5911586998130207\n",
      "Epoch 83 / 100 ------> 0.5911586998130206\n",
      "Epoch 84 / 100 ------> 0.5911586998130206\n",
      "Epoch 85 / 100 ------> 0.5911586998130206\n",
      "Epoch 86 / 100 ------> 0.5911586998130206\n",
      "Epoch 87 / 100 ------> 0.5911586998130206\n",
      "Epoch 88 / 100 ------> 0.5911586998130206\n",
      "Epoch 89 / 100 ------> 0.5911586998130207\n",
      "Epoch 90 / 100 ------> 0.5911586998130206\n",
      "Epoch 91 / 100 ------> 0.5911586998130206\n",
      "Epoch 92 / 100 ------> 0.5911586998130206\n",
      "Epoch 93 / 100 ------> 0.5911586998130206\n",
      "Epoch 94 / 100 ------> 0.5911586998130206\n",
      "Epoch 95 / 100 ------> 0.5911586998130206\n",
      "Epoch 96 / 100 ------> 0.5911586998130206\n",
      "Epoch 97 / 100 ------> 0.5911586998130206\n",
      "Epoch 98 / 100 ------> 0.5911586998130206\n",
      "Epoch 99 / 100 ------> 0.5911586998130206\n",
      "Epoch 100 / 100 ------> 0.5911586998130206\n",
      "mbgd_loss_record [18.87208318824271, 2.73180985220011, 0.838492394853013, 0.6198808968356218, 0.5945208719315535, 0.5915548375381656, 0.5912056079118679, 0.5911642757153739, 0.5911593645668601, 0.5911587792447626, 0.591158709321209, 0.5911587009527722, 0.591158699949797, 0.5911586998294496, 0.5911586998149955, 0.5911586998132581, 0.5911586998130491, 0.591158699813024, 0.5911586998130209, 0.5911586998130206, 0.5911586998130206, 0.5911586998130205, 0.5911586998130206, 0.5911586998130207, 0.5911586998130206, 0.5911586998130205, 0.5911586998130207, 0.5911586998130205, 0.5911586998130205, 0.5911586998130206, 0.5911586998130205, 0.5911586998130205, 0.5911586998130205, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130205, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130205, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130207, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130207, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130207, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206, 0.5911586998130206]\n",
      "length of mbgd_loss_record -> 100\n",
      "[[ 0.03016043]\n",
      " [ 0.01934448]\n",
      " [ 0.03077303]\n",
      " [ 0.02776886]\n",
      " [ 0.0195885 ]\n",
      " [ 0.02713652]\n",
      " [ 0.02618415]\n",
      " [ 0.03064698]\n",
      " [ 0.00767674]\n",
      " [ 0.00275412]\n",
      " [ 0.01560727]\n",
      " [-0.00638683]\n",
      " [ 0.0181875 ]\n",
      " [ 0.01727513]\n",
      " [-0.00241173]\n",
      " [ 0.0060912 ]\n",
      " [ 0.00190221]\n",
      " [ 0.01221254]\n",
      " [-0.01189611]\n",
      " [-0.00252806]\n",
      " [ 0.03253208]\n",
      " [ 0.02177067]\n",
      " [ 0.03388292]\n",
      " [ 0.03028251]\n",
      " [ 0.02281984]\n",
      " [ 0.02944568]\n",
      " [ 0.02803129]\n",
      " [ 0.03447173]\n",
      " [ 0.01360116]\n",
      " [ 0.0190105 ]] (30, 1)\n"
     ]
    }
   ],
   "source": [
    "#MBGD parameter Tunning\n",
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "x_train_array = x_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "w = numpy.ones((30,1))\n",
    "mbgd_weights_reg,mbgd_loss_record_reg = mbgd(x_train_array,y_train_array,10,0.01,w,100)\n",
    "print(mbgd_weights_reg,mbgd_weights_reg.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
